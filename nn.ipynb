{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "731f65f2-87a5-4669-8a2d-7a01acd24cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01a57831-1703-4f0d-901d-7962afce5f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "events_df = pd.read_csv('./data/events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "750e5cd3-9804-48d7-b1c1-d858aa550968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamps to datetime for better readability\n",
    "events_df['timestamp'] = pd.to_datetime(events_df['timestamp'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aeed3c24-da3b-48d5-bafd-cbdd22b0d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only sample 10,000 interactions\n",
    "sampled_events_df = events_df.sample(n=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65e76aaf-7c59-413d-a2ba-43ee2c87f97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split date: 2015-08-18 15:23:50.822800128\n"
     ]
    }
   ],
   "source": [
    "# Define a split date (example: 80% of the data before the split date for training, 20% after for testing)\n",
    "split_date = sampled_events_df['timestamp'].quantile(0.8)\n",
    "print(\"Split date:\", split_date)\n",
    "train_df = sampled_events_df[sampled_events_df['timestamp'] < split_date]\n",
    "test_df = sampled_events_df[sampled_events_df['timestamp'] >= split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a292407b-abf0-4648-817b-21e275d9391e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of interactions in the training set: 80000\n",
      "Number of interactions in the test set: 20000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of interactions in the training set:\", len(train_df))\n",
    "print(\"Number of interactions in the test set:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "185a8215-175e-42df-8dc5-6216ca297fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider only unique pairs (user, item)\n",
    "train_df = train_df.drop_duplicates(subset=['visitorid', 'itemid'])\n",
    "test_df = test_df.drop_duplicates(subset=['visitorid', 'itemid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2284871-073e-4e4f-8149-f5ffc9225ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique (user, item) pairs in the training set: 78342\n",
      "Number of unique (user, item) pairs in the test set: 19655\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique (user, item) pairs in the training set:\", len(train_df))\n",
    "print(\"Number of unique (user, item) pairs in the test set:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c40ce64-0b6b-4e0b-b4bb-521a8b1a3083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that all users in the test set are also in the training set\n",
    "test_df = test_df[test_df['visitorid'].isin(train_df['visitorid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "698e247d-91f6-4771-8555-2b595e55dc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conjunto de teste após filtrar usuários e itens não vistos:\n",
      "                       timestamp  visitorid      event  itemid  transactionid\n",
      "1223771 2015-09-04 15:45:22.989     914082       view  105108            NaN\n",
      "1149787 2015-08-31 20:33:47.398     899857       view   76512            NaN\n",
      "1380228 2015-09-12 18:06:25.336     530559  addtocart  277490            NaN\n",
      "1024747 2015-08-24 22:24:25.585     247235       view  395749            NaN\n",
      "1434974 2015-09-15 16:56:46.717     276250       view  411126            NaN\n"
     ]
    }
   ],
   "source": [
    "# Ensure that all items in the test set are also in the training set\n",
    "test_df = test_df[test_df['itemid'].isin(train_df['itemid'])]\n",
    "print(\"\\nConjunto de teste após filtrar usuários e itens não vistos:\\n\", test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b19e8ce4-148f-44d7-a0eb-4ceb81803fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar IDs de usuário e item\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "937eb3d5-b4be-403c-9593-c573461665f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create explicit copies of the DataFrame to avoid SettingWithCopyWa\n",
    "train_df_copy = train_df.copy()\n",
    "test_df_copy = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ca0e858-15b7-4444-a310-e56f84b4f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust and transform training data\n",
    "train_df_copy['user'] = user_encoder.fit_transform(train_df_copy['visitorid'])\n",
    "train_df_copy['item'] = item_encoder.fit_transform(train_df_copy['itemid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "862913b3-4cd8-4c41-9c3c-a345a670f8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conjunto de treinamento codificado:\n",
      "                       timestamp  visitorid event  itemid  transactionid  \\\n",
      "486798  2015-06-25 00:46:56.976      50734  view    4442            NaN   \n",
      "1601366 2015-05-10 17:50:37.515    1066758  view  221329            NaN   \n",
      "843976  2015-08-13 23:17:44.399    1049477  view   23683            NaN   \n",
      "2524686 2015-07-22 21:39:19.324     143239  view    6552            NaN   \n",
      "2361757 2015-07-14 21:27:06.237     976898  view   82224            NaN   \n",
      "\n",
      "          user   item  \n",
      "486798    2534    386  \n",
      "1601366  53091  20145  \n",
      "843976   52184   2123  \n",
      "2524686   7167    582  \n",
      "2361757  48592   7530  \n",
      "\n",
      "Mapeamento de usuários (visitorid -> user):\n",
      "{50734: 2534, 1066758: 53091, 1049477: 52184, 143239: 7167, 976898: 48592}\n",
      "\n",
      "Mapeamento de itens (itemid -> item):\n",
      "{4442: 386, 221329: 20145, 23683: 2123, 6552: 582, 82224: 7530}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConjunto de treinamento codificado:\\n\", train_df_copy.head())\n",
    "\n",
    "print(\"\\nMapeamento de usuários (visitorid -> user):\")\n",
    "print(dict(zip(train_df_copy['visitorid'].head(), train_df_copy['user'].head())))\n",
    "\n",
    "print(\"\\nMapeamento de itens (itemid -> item):\")\n",
    "print(dict(zip(train_df_copy['itemid'].head(), train_df_copy['item'].head())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f9b325e-65f4-48f3-b599-7c76e0950ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conjunto de teste codificado:\n",
      "                       timestamp  visitorid      event  itemid  transactionid  \\\n",
      "1223771 2015-09-04 15:45:22.989     914082       view  105108            NaN   \n",
      "1149787 2015-08-31 20:33:47.398     899857       view   76512            NaN   \n",
      "1380228 2015-09-12 18:06:25.336     530559  addtocart  277490            NaN   \n",
      "1024747 2015-08-24 22:24:25.585     247235       view  395749            NaN   \n",
      "1434974 2015-09-15 16:56:46.717     276250       view  411126            NaN   \n",
      "\n",
      "          user   item  \n",
      "1223771  45476   9628  \n",
      "1149787  44790   6975  \n",
      "1380228  26548  25408  \n",
      "1024747  12436  36301  \n",
      "1434974  13889  37611  \n"
     ]
    }
   ],
   "source": [
    "# Transform test data using the same encoder\n",
    "test_df_copy['user'] = user_encoder.transform(test_df_copy['visitorid'])\n",
    "test_df_copy['item'] = item_encoder.transform(test_df_copy['itemid'])\n",
    "\n",
    "print(\"\\nConjunto de teste codificado:\\n\", test_df_copy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a5f8bb0a-7df8-42eb-883b-046a88c696ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset for PyTorch\n",
    "class InteractionDataset(Dataset):\n",
    "    def __init__(self, users, items, labels):\n",
    "        self.users = users\n",
    "        self.items = items\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d77f8c4-c7a3-40d7-b8ae-729b1bf2fa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate positive and negative samples for training\n",
    "def generate_samples(df, num_negatives=1):\n",
    "    user_item_set = set(zip(df['user'], df['item']))\n",
    "    all_items = df['item'].unique()\n",
    "\n",
    "    positive_samples = df[['user', 'item']].drop_duplicates()\n",
    "    positive_samples['label'] = 1\n",
    "\n",
    "    negative_samples = []\n",
    "    for user in df['user'].unique():\n",
    "        negative_items = np.random.choice(all_items, size=num_negatives * len(df[df['user'] == user]), replace=True)\n",
    "        for item in negative_items:\n",
    "            if (user, item) not in user_item_set:\n",
    "                negative_samples.append((user, item, 0))\n",
    "\n",
    "    negative_samples_df = pd.DataFrame(negative_samples, columns=['user', 'item', 'label'])\n",
    "    samples = pd.concat([positive_samples, negative_samples_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return samples['user'].values, samples['item'].values, samples['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fad09d55-bb6b-434a-b822-de6cc2dfae28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amostras de treinamento:\n",
      "\n",
      "Usuários: [  239 44551 24850 16593 64381]\n",
      "Itens: [25459 20676  7936 16160 17315]\n",
      "Labels: [1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Generate samples for training\n",
    "train_users, train_items, train_labels = generate_samples(train_df_copy)\n",
    "\n",
    "print(\"\\nAmostras de treinamento:\\n\")\n",
    "print(\"Usuários:\", train_users[:5])\n",
    "print(\"Itens:\", train_items[:5])\n",
    "print(\"Labels:\", train_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe48a831-69df-4881-855c-f94fbf6f5777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training\n",
    "train_dataset = InteractionDataset(train_users, train_items, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7f934fa-f80b-4d51-aa59-2e079cb68729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class SimpleNeuralCollaborativeFiltering(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_size=5):  # Reduzindo ainda mais o tamanho da embedding\n",
    "        super(SimpleNeuralCollaborativeFiltering, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_size)\n",
    "        self.fc1 = nn.Linear(embedding_size * 2, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        user_embedded = self.user_embedding(user)\n",
    "        item_embedded = self.item_embedding(item)\n",
    "        x = torch.cat([user_embedded, item_embedded], dim=-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84def16f-68f5-4c44-8a0c-d314047ae655",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(user_encoder.classes_)\n",
    "num_items = len(item_encoder.classes_)\n",
    "model = SimpleNeuralCollaborativeFiltering(num_users, num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7300e353-7081-4a0b-bea1-1c61943c6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72368551-488f-4085-9595-f499cb1a54e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNeuralCollaborativeFiltering(\n",
       "  (user_embedding): Embedding(69990, 5)\n",
       "  (item_embedding): Embedding(42628, 5)\n",
       "  (fc1): Linear(in_features=10, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (fc3): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training settings\n",
    "device = torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58a0cb99-47bf-46dd-84e3-cb7c9fdc8ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/4897, Loss: 0.6885088682174683\n",
      "Batch 10/4897, Loss: 0.6953601837158203\n",
      "Batch 20/4897, Loss: 0.6951708793640137\n",
      "Batch 30/4897, Loss: 0.6869214177131653\n",
      "Batch 40/4897, Loss: 0.6833251118659973\n",
      "Batch 50/4897, Loss: 0.6968539357185364\n",
      "Batch 60/4897, Loss: 0.6945077180862427\n",
      "Batch 70/4897, Loss: 0.6935948133468628\n",
      "Batch 80/4897, Loss: 0.6943491697311401\n",
      "Batch 90/4897, Loss: 0.6972706317901611\n",
      "Batch 100/4897, Loss: 0.6997047662734985\n",
      "Batch 110/4897, Loss: 0.6999765634536743\n",
      "Batch 120/4897, Loss: 0.6961562037467957\n",
      "Batch 130/4897, Loss: 0.6889839768409729\n",
      "Batch 140/4897, Loss: 0.6952756643295288\n",
      "Batch 150/4897, Loss: 0.6885349750518799\n",
      "Batch 160/4897, Loss: 0.6911840438842773\n",
      "Batch 170/4897, Loss: 0.6964337229728699\n",
      "Batch 180/4897, Loss: 0.6913577318191528\n",
      "Batch 190/4897, Loss: 0.6850990056991577\n",
      "Batch 200/4897, Loss: 0.6912617683410645\n",
      "Batch 210/4897, Loss: 0.6935731172561646\n",
      "Batch 220/4897, Loss: 0.7000172734260559\n",
      "Batch 230/4897, Loss: 0.6913942694664001\n",
      "Batch 240/4897, Loss: 0.6969150900840759\n",
      "Batch 250/4897, Loss: 0.6928020715713501\n",
      "Batch 260/4897, Loss: 0.6984071731567383\n",
      "Batch 270/4897, Loss: 0.6911385655403137\n",
      "Batch 280/4897, Loss: 0.6898064613342285\n",
      "Batch 290/4897, Loss: 0.691407322883606\n",
      "Batch 300/4897, Loss: 0.7007882595062256\n",
      "Batch 310/4897, Loss: 0.693576455116272\n",
      "Batch 320/4897, Loss: 0.6893835067749023\n",
      "Batch 330/4897, Loss: 0.690183162689209\n",
      "Batch 340/4897, Loss: 0.6986002326011658\n",
      "Batch 350/4897, Loss: 0.6930937767028809\n",
      "Batch 360/4897, Loss: 0.6923707127571106\n",
      "Batch 370/4897, Loss: 0.6977895498275757\n",
      "Batch 380/4897, Loss: 0.6902703046798706\n",
      "Batch 390/4897, Loss: 0.7007767558097839\n",
      "Batch 400/4897, Loss: 0.6951216459274292\n",
      "Batch 410/4897, Loss: 0.6928775310516357\n",
      "Batch 420/4897, Loss: 0.6927891969680786\n",
      "Batch 430/4897, Loss: 0.6921796202659607\n",
      "Batch 440/4897, Loss: 0.699958324432373\n",
      "Batch 450/4897, Loss: 0.7069770097732544\n",
      "Batch 460/4897, Loss: 0.6863967776298523\n",
      "Batch 470/4897, Loss: 0.6934804320335388\n",
      "Batch 480/4897, Loss: 0.6916518211364746\n",
      "Batch 490/4897, Loss: 0.6864758133888245\n",
      "Batch 500/4897, Loss: 0.688446044921875\n",
      "Batch 510/4897, Loss: 0.6970324516296387\n",
      "Batch 520/4897, Loss: 0.6999271512031555\n",
      "Batch 530/4897, Loss: 0.6914377808570862\n",
      "Batch 540/4897, Loss: 0.6926275491714478\n",
      "Batch 550/4897, Loss: 0.6942445039749146\n",
      "Batch 560/4897, Loss: 0.6908061504364014\n",
      "Batch 570/4897, Loss: 0.6961997747421265\n",
      "Batch 580/4897, Loss: 0.6933896541595459\n",
      "Batch 590/4897, Loss: 0.6930290460586548\n",
      "Batch 600/4897, Loss: 0.6918561458587646\n",
      "Batch 610/4897, Loss: 0.6939523220062256\n",
      "Batch 620/4897, Loss: 0.6954143047332764\n",
      "Batch 630/4897, Loss: 0.6925979852676392\n",
      "Batch 640/4897, Loss: 0.6921270489692688\n",
      "Batch 650/4897, Loss: 0.6932072639465332\n",
      "Batch 660/4897, Loss: 0.6896618008613586\n",
      "Batch 670/4897, Loss: 0.6934554576873779\n",
      "Batch 680/4897, Loss: 0.6888377070426941\n",
      "Batch 690/4897, Loss: 0.6854740381240845\n",
      "Batch 700/4897, Loss: 0.6910901069641113\n",
      "Batch 710/4897, Loss: 0.68918776512146\n",
      "Batch 720/4897, Loss: 0.6885391473770142\n",
      "Batch 730/4897, Loss: 0.6904087066650391\n",
      "Batch 740/4897, Loss: 0.6950978636741638\n",
      "Batch 750/4897, Loss: 0.6878864169120789\n",
      "Batch 760/4897, Loss: 0.6987998485565186\n",
      "Batch 770/4897, Loss: 0.706109881401062\n",
      "Batch 780/4897, Loss: 0.7037105560302734\n",
      "Batch 790/4897, Loss: 0.6903604865074158\n",
      "Batch 800/4897, Loss: 0.6957493424415588\n",
      "Batch 810/4897, Loss: 0.697957456111908\n",
      "Batch 820/4897, Loss: 0.6899473071098328\n",
      "Batch 830/4897, Loss: 0.6903589963912964\n",
      "Batch 840/4897, Loss: 0.6972797513008118\n",
      "Batch 850/4897, Loss: 0.6857541799545288\n",
      "Batch 860/4897, Loss: 0.6926836371421814\n",
      "Batch 870/4897, Loss: 0.691146969795227\n",
      "Batch 880/4897, Loss: 0.6922617554664612\n",
      "Batch 890/4897, Loss: 0.6915614008903503\n",
      "Batch 900/4897, Loss: 0.6916426420211792\n",
      "Batch 910/4897, Loss: 0.6925812363624573\n",
      "Batch 920/4897, Loss: 0.692672073841095\n",
      "Batch 930/4897, Loss: 0.6927168369293213\n",
      "Batch 940/4897, Loss: 0.695518970489502\n",
      "Batch 950/4897, Loss: 0.6943877935409546\n",
      "Batch 960/4897, Loss: 0.6918363571166992\n",
      "Batch 970/4897, Loss: 0.6892397403717041\n",
      "Batch 980/4897, Loss: 0.6939493417739868\n",
      "Batch 990/4897, Loss: 0.6926259398460388\n",
      "Batch 1000/4897, Loss: 0.6890090703964233\n",
      "Batch 1010/4897, Loss: 0.6927268505096436\n",
      "Batch 1020/4897, Loss: 0.6921951174736023\n",
      "Batch 1030/4897, Loss: 0.6940497756004333\n",
      "Batch 1040/4897, Loss: 0.6928229331970215\n",
      "Batch 1050/4897, Loss: 0.6906837224960327\n",
      "Batch 1060/4897, Loss: 0.6968355774879456\n",
      "Batch 1070/4897, Loss: 0.690610945224762\n",
      "Batch 1080/4897, Loss: 0.6955796480178833\n",
      "Batch 1090/4897, Loss: 0.6905379891395569\n",
      "Batch 1100/4897, Loss: 0.6941350102424622\n",
      "Batch 1110/4897, Loss: 0.6899107694625854\n",
      "Batch 1120/4897, Loss: 0.693658709526062\n",
      "Batch 1130/4897, Loss: 0.6967764496803284\n",
      "Batch 1140/4897, Loss: 0.6894013285636902\n",
      "Batch 1150/4897, Loss: 0.6957615613937378\n",
      "Batch 1160/4897, Loss: 0.693722665309906\n",
      "Batch 1170/4897, Loss: 0.6913516521453857\n",
      "Batch 1180/4897, Loss: 0.6936512589454651\n",
      "Batch 1190/4897, Loss: 0.6926199197769165\n",
      "Batch 1200/4897, Loss: 0.6943438053131104\n",
      "Batch 1210/4897, Loss: 0.6944092512130737\n",
      "Batch 1220/4897, Loss: 0.6940726637840271\n",
      "Batch 1230/4897, Loss: 0.6922258138656616\n",
      "Batch 1240/4897, Loss: 0.6931445598602295\n",
      "Batch 1250/4897, Loss: 0.6936154961585999\n",
      "Batch 1260/4897, Loss: 0.6929522752761841\n",
      "Batch 1270/4897, Loss: 0.6906669735908508\n",
      "Batch 1280/4897, Loss: 0.6931240558624268\n",
      "Batch 1290/4897, Loss: 0.6930788159370422\n",
      "Batch 1300/4897, Loss: 0.6934141516685486\n",
      "Batch 1310/4897, Loss: 0.6918061971664429\n",
      "Batch 1320/4897, Loss: 0.6939910054206848\n",
      "Batch 1330/4897, Loss: 0.6915101408958435\n",
      "Batch 1340/4897, Loss: 0.6969310641288757\n",
      "Batch 1350/4897, Loss: 0.6950902342796326\n",
      "Batch 1360/4897, Loss: 0.6928040385246277\n",
      "Batch 1370/4897, Loss: 0.6906511783599854\n",
      "Batch 1380/4897, Loss: 0.6951274275779724\n",
      "Batch 1390/4897, Loss: 0.6949847340583801\n",
      "Batch 1400/4897, Loss: 0.6977733969688416\n",
      "Batch 1410/4897, Loss: 0.6946538090705872\n",
      "Batch 1420/4897, Loss: 0.6930724382400513\n",
      "Batch 1430/4897, Loss: 0.6949871182441711\n",
      "Batch 1440/4897, Loss: 0.6921935677528381\n",
      "Batch 1450/4897, Loss: 0.6941176056861877\n",
      "Batch 1460/4897, Loss: 0.6952348351478577\n",
      "Batch 1470/4897, Loss: 0.694061815738678\n",
      "Batch 1480/4897, Loss: 0.6952197551727295\n",
      "Batch 1490/4897, Loss: 0.693182110786438\n",
      "Batch 1500/4897, Loss: 0.690815269947052\n",
      "Batch 1510/4897, Loss: 0.6934235095977783\n",
      "Batch 1520/4897, Loss: 0.6937572360038757\n",
      "Batch 1530/4897, Loss: 0.693018913269043\n",
      "Batch 1540/4897, Loss: 0.6994926929473877\n",
      "Batch 1550/4897, Loss: 0.6950607299804688\n",
      "Batch 1560/4897, Loss: 0.6918239593505859\n",
      "Batch 1570/4897, Loss: 0.6915144920349121\n",
      "Batch 1580/4897, Loss: 0.6939082741737366\n",
      "Batch 1590/4897, Loss: 0.6893633604049683\n",
      "Batch 1600/4897, Loss: 0.6980538964271545\n",
      "Batch 1610/4897, Loss: 0.6949825286865234\n",
      "Batch 1620/4897, Loss: 0.6855112314224243\n",
      "Batch 1630/4897, Loss: 0.6997083425521851\n",
      "Batch 1640/4897, Loss: 0.6878618001937866\n",
      "Batch 1650/4897, Loss: 0.7010756134986877\n",
      "Batch 1660/4897, Loss: 0.7014985680580139\n",
      "Batch 1670/4897, Loss: 0.6978702545166016\n",
      "Batch 1680/4897, Loss: 0.6918684244155884\n",
      "Batch 1690/4897, Loss: 0.6982088685035706\n",
      "Batch 1700/4897, Loss: 0.6984857320785522\n",
      "Batch 1710/4897, Loss: 0.6899110078811646\n",
      "Batch 1720/4897, Loss: 0.6935936808586121\n",
      "Batch 1730/4897, Loss: 0.6942387223243713\n",
      "Batch 1740/4897, Loss: 0.6901545524597168\n",
      "Batch 1750/4897, Loss: 0.6927388906478882\n",
      "Batch 1760/4897, Loss: 0.6897518634796143\n",
      "Batch 1770/4897, Loss: 0.6940601468086243\n",
      "Batch 1780/4897, Loss: 0.6922850012779236\n",
      "Batch 1790/4897, Loss: 0.6938024163246155\n",
      "Batch 1800/4897, Loss: 0.6942797899246216\n",
      "Batch 1810/4897, Loss: 0.6922953128814697\n",
      "Batch 1820/4897, Loss: 0.6919350624084473\n",
      "Batch 1830/4897, Loss: 0.6919190883636475\n",
      "Batch 1840/4897, Loss: 0.691207766532898\n",
      "Batch 1850/4897, Loss: 0.6939647793769836\n",
      "Batch 1860/4897, Loss: 0.6950663328170776\n",
      "Batch 1870/4897, Loss: 0.6921140551567078\n",
      "Batch 1880/4897, Loss: 0.6933068037033081\n",
      "Batch 1890/4897, Loss: 0.6928308606147766\n",
      "Batch 1900/4897, Loss: 0.6938594579696655\n",
      "Batch 1910/4897, Loss: 0.693707287311554\n",
      "Batch 1920/4897, Loss: 0.6907888054847717\n",
      "Batch 1930/4897, Loss: 0.6966874003410339\n",
      "Batch 1940/4897, Loss: 0.6934473514556885\n",
      "Batch 1950/4897, Loss: 0.6950076818466187\n",
      "Batch 1960/4897, Loss: 0.6927026510238647\n",
      "Batch 1970/4897, Loss: 0.6938892006874084\n",
      "Batch 1980/4897, Loss: 0.6915503144264221\n",
      "Batch 1990/4897, Loss: 0.6906231045722961\n",
      "Batch 2000/4897, Loss: 0.6964430809020996\n",
      "Batch 2010/4897, Loss: 0.6919589042663574\n",
      "Batch 2020/4897, Loss: 0.6936705112457275\n",
      "Batch 2030/4897, Loss: 0.6941500306129456\n",
      "Batch 2040/4897, Loss: 0.6917337775230408\n",
      "Batch 2050/4897, Loss: 0.6928115487098694\n",
      "Batch 2060/4897, Loss: 0.6914227604866028\n",
      "Batch 2070/4897, Loss: 0.6934586763381958\n",
      "Batch 2080/4897, Loss: 0.6926941871643066\n",
      "Batch 2090/4897, Loss: 0.6896191239356995\n",
      "Batch 2100/4897, Loss: 0.6925899982452393\n",
      "Batch 2110/4897, Loss: 0.6959237456321716\n",
      "Batch 2120/4897, Loss: 0.6909273862838745\n",
      "Batch 2130/4897, Loss: 0.6911858916282654\n",
      "Batch 2140/4897, Loss: 0.6906839609146118\n",
      "Batch 2150/4897, Loss: 0.6926759481430054\n",
      "Batch 2160/4897, Loss: 0.6993358135223389\n",
      "Batch 2170/4897, Loss: 0.6850845813751221\n",
      "Batch 2180/4897, Loss: 0.6943925023078918\n",
      "Batch 2190/4897, Loss: 0.6913975477218628\n",
      "Batch 2200/4897, Loss: 0.6938961744308472\n",
      "Batch 2210/4897, Loss: 0.6933143734931946\n",
      "Batch 2220/4897, Loss: 0.6955087780952454\n",
      "Batch 2230/4897, Loss: 0.6922887563705444\n",
      "Batch 2240/4897, Loss: 0.6934375762939453\n",
      "Batch 2250/4897, Loss: 0.6928967237472534\n",
      "Batch 2260/4897, Loss: 0.6912647485733032\n",
      "Batch 2270/4897, Loss: 0.6938548684120178\n",
      "Batch 2280/4897, Loss: 0.6889445185661316\n",
      "Batch 2290/4897, Loss: 0.6928454041481018\n",
      "Batch 2300/4897, Loss: 0.6971496343612671\n",
      "Batch 2310/4897, Loss: 0.6963025331497192\n",
      "Batch 2320/4897, Loss: 0.6948431730270386\n",
      "Batch 2330/4897, Loss: 0.6942077875137329\n",
      "Batch 2340/4897, Loss: 0.6940277218818665\n",
      "Batch 2350/4897, Loss: 0.6943075060844421\n",
      "Batch 2360/4897, Loss: 0.6909475922584534\n",
      "Batch 2370/4897, Loss: 0.6944469809532166\n",
      "Batch 2380/4897, Loss: 0.6911695003509521\n",
      "Batch 2390/4897, Loss: 0.6934987902641296\n",
      "Batch 2400/4897, Loss: 0.6937319040298462\n",
      "Batch 2410/4897, Loss: 0.6947957873344421\n",
      "Batch 2420/4897, Loss: 0.6937069892883301\n",
      "Batch 2430/4897, Loss: 0.6903458833694458\n",
      "Batch 2440/4897, Loss: 0.6901201605796814\n",
      "Batch 2450/4897, Loss: 0.6937215924263\n",
      "Batch 2460/4897, Loss: 0.6907679438591003\n",
      "Batch 2470/4897, Loss: 0.6948845386505127\n",
      "Batch 2480/4897, Loss: 0.6926504969596863\n",
      "Batch 2490/4897, Loss: 0.691887617111206\n",
      "Batch 2500/4897, Loss: 0.6927071809768677\n",
      "Batch 2510/4897, Loss: 0.6965678930282593\n",
      "Batch 2520/4897, Loss: 0.6934817433357239\n",
      "Batch 2530/4897, Loss: 0.6888338327407837\n",
      "Batch 2540/4897, Loss: 0.692761242389679\n",
      "Batch 2550/4897, Loss: 0.6981072425842285\n",
      "Batch 2560/4897, Loss: 0.6934223175048828\n",
      "Batch 2570/4897, Loss: 0.6920199990272522\n",
      "Batch 2580/4897, Loss: 0.6938207149505615\n",
      "Batch 2590/4897, Loss: 0.6944215297698975\n",
      "Batch 2600/4897, Loss: 0.6936461925506592\n",
      "Batch 2610/4897, Loss: 0.6964715719223022\n",
      "Batch 2620/4897, Loss: 0.692314863204956\n",
      "Batch 2630/4897, Loss: 0.6916922926902771\n",
      "Batch 2640/4897, Loss: 0.6947977542877197\n",
      "Batch 2650/4897, Loss: 0.6926611661911011\n",
      "Batch 2660/4897, Loss: 0.6896137595176697\n",
      "Batch 2670/4897, Loss: 0.6916790008544922\n",
      "Batch 2680/4897, Loss: 0.6906746029853821\n",
      "Batch 2690/4897, Loss: 0.6921155452728271\n",
      "Batch 2700/4897, Loss: 0.6928936839103699\n",
      "Batch 2710/4897, Loss: 0.6930285096168518\n",
      "Batch 2720/4897, Loss: 0.6957548260688782\n",
      "Batch 2730/4897, Loss: 0.6906344890594482\n",
      "Batch 2740/4897, Loss: 0.693298876285553\n",
      "Batch 2750/4897, Loss: 0.6947289705276489\n",
      "Batch 2760/4897, Loss: 0.6902649402618408\n",
      "Batch 2770/4897, Loss: 0.6937891244888306\n",
      "Batch 2780/4897, Loss: 0.690574049949646\n",
      "Batch 2790/4897, Loss: 0.6938495635986328\n",
      "Batch 2800/4897, Loss: 0.6923888325691223\n",
      "Batch 2810/4897, Loss: 0.6949033737182617\n",
      "Batch 2820/4897, Loss: 0.691345751285553\n",
      "Batch 2830/4897, Loss: 0.6973085403442383\n",
      "Batch 2840/4897, Loss: 0.6932634115219116\n",
      "Batch 2850/4897, Loss: 0.6932758688926697\n",
      "Batch 2860/4897, Loss: 0.6944562196731567\n",
      "Batch 2870/4897, Loss: 0.6922369599342346\n",
      "Batch 2880/4897, Loss: 0.6930750012397766\n",
      "Batch 2890/4897, Loss: 0.6957904696464539\n",
      "Batch 2900/4897, Loss: 0.6945817470550537\n",
      "Batch 2910/4897, Loss: 0.6937918663024902\n",
      "Batch 2920/4897, Loss: 0.6939147710800171\n",
      "Batch 2930/4897, Loss: 0.6931174993515015\n",
      "Batch 2940/4897, Loss: 0.6938939094543457\n",
      "Batch 2950/4897, Loss: 0.6932076215744019\n",
      "Batch 2960/4897, Loss: 0.6938750147819519\n",
      "Batch 2970/4897, Loss: 0.692476749420166\n",
      "Batch 2980/4897, Loss: 0.6921840310096741\n",
      "Batch 2990/4897, Loss: 0.6936857104301453\n",
      "Batch 3000/4897, Loss: 0.6933861970901489\n",
      "Batch 3010/4897, Loss: 0.6907911896705627\n",
      "Batch 3020/4897, Loss: 0.6930835843086243\n",
      "Batch 3030/4897, Loss: 0.6949719190597534\n",
      "Batch 3040/4897, Loss: 0.6933584809303284\n",
      "Batch 3050/4897, Loss: 0.6935757994651794\n",
      "Batch 3060/4897, Loss: 0.6931064128875732\n",
      "Batch 3070/4897, Loss: 0.6912913918495178\n",
      "Batch 3080/4897, Loss: 0.6937583684921265\n",
      "Batch 3090/4897, Loss: 0.6942266225814819\n",
      "Batch 3100/4897, Loss: 0.6908309459686279\n",
      "Batch 3110/4897, Loss: 0.6915345788002014\n",
      "Batch 3120/4897, Loss: 0.694137454032898\n",
      "Batch 3130/4897, Loss: 0.6935527324676514\n",
      "Batch 3140/4897, Loss: 0.6930639743804932\n",
      "Batch 3150/4897, Loss: 0.6904664635658264\n",
      "Batch 3160/4897, Loss: 0.6927036643028259\n",
      "Batch 3170/4897, Loss: 0.6951514482498169\n",
      "Batch 3180/4897, Loss: 0.6933770775794983\n",
      "Batch 3190/4897, Loss: 0.693324625492096\n",
      "Batch 3200/4897, Loss: 0.6913235187530518\n",
      "Batch 3210/4897, Loss: 0.689410924911499\n",
      "Batch 3220/4897, Loss: 0.69105064868927\n",
      "Batch 3230/4897, Loss: 0.6932955980300903\n",
      "Batch 3240/4897, Loss: 0.6927311420440674\n",
      "Batch 3250/4897, Loss: 0.6973490715026855\n",
      "Batch 3260/4897, Loss: 0.6956469416618347\n",
      "Batch 3270/4897, Loss: 0.6931434869766235\n",
      "Batch 3280/4897, Loss: 0.6947627663612366\n",
      "Batch 3290/4897, Loss: 0.6936684250831604\n",
      "Batch 3300/4897, Loss: 0.6953411102294922\n",
      "Batch 3310/4897, Loss: 0.6929086446762085\n",
      "Batch 3320/4897, Loss: 0.6930901408195496\n",
      "Batch 3330/4897, Loss: 0.6924187541007996\n",
      "Batch 3340/4897, Loss: 0.6951863765716553\n",
      "Batch 3350/4897, Loss: 0.6937099099159241\n",
      "Batch 3360/4897, Loss: 0.691210150718689\n",
      "Batch 3370/4897, Loss: 0.6932840943336487\n",
      "Batch 3380/4897, Loss: 0.6942789554595947\n",
      "Batch 3390/4897, Loss: 0.6952098608016968\n",
      "Batch 3400/4897, Loss: 0.6937890648841858\n",
      "Batch 3410/4897, Loss: 0.6939078569412231\n",
      "Batch 3420/4897, Loss: 0.6947739124298096\n",
      "Batch 3430/4897, Loss: 0.6952223777770996\n",
      "Batch 3440/4897, Loss: 0.694353461265564\n",
      "Batch 3450/4897, Loss: 0.6925630569458008\n",
      "Batch 3460/4897, Loss: 0.6923054456710815\n",
      "Batch 3470/4897, Loss: 0.6934317350387573\n",
      "Batch 3480/4897, Loss: 0.6930415630340576\n",
      "Batch 3490/4897, Loss: 0.6951472163200378\n",
      "Batch 3500/4897, Loss: 0.6945951581001282\n",
      "Batch 3510/4897, Loss: 0.6932789087295532\n",
      "Batch 3520/4897, Loss: 0.6930615305900574\n",
      "Batch 3530/4897, Loss: 0.6946566700935364\n",
      "Batch 3540/4897, Loss: 0.6928963661193848\n",
      "Batch 3550/4897, Loss: 0.6949073076248169\n",
      "Batch 3560/4897, Loss: 0.6934224367141724\n",
      "Batch 3570/4897, Loss: 0.6932239532470703\n",
      "Batch 3580/4897, Loss: 0.6927835941314697\n",
      "Batch 3590/4897, Loss: 0.6940914988517761\n",
      "Batch 3600/4897, Loss: 0.6927331686019897\n",
      "Batch 3610/4897, Loss: 0.6928251385688782\n",
      "Batch 3620/4897, Loss: 0.692392885684967\n",
      "Batch 3630/4897, Loss: 0.6932339072227478\n",
      "Batch 3640/4897, Loss: 0.6925562620162964\n",
      "Batch 3650/4897, Loss: 0.6936133503913879\n",
      "Batch 3660/4897, Loss: 0.6933403611183167\n",
      "Batch 3670/4897, Loss: 0.6927365660667419\n",
      "Batch 3680/4897, Loss: 0.6927187442779541\n",
      "Batch 3690/4897, Loss: 0.6925329566001892\n",
      "Batch 3700/4897, Loss: 0.6934764385223389\n",
      "Batch 3710/4897, Loss: 0.691630482673645\n",
      "Batch 3720/4897, Loss: 0.6933820247650146\n",
      "Batch 3730/4897, Loss: 0.6973621249198914\n",
      "Batch 3740/4897, Loss: 0.6936401128768921\n",
      "Batch 3750/4897, Loss: 0.693964421749115\n",
      "Batch 3760/4897, Loss: 0.6946384906768799\n",
      "Batch 3770/4897, Loss: 0.694643497467041\n",
      "Batch 3780/4897, Loss: 0.6949157118797302\n",
      "Batch 3790/4897, Loss: 0.693950891494751\n",
      "Batch 3800/4897, Loss: 0.6955548524856567\n",
      "Batch 3810/4897, Loss: 0.6942136287689209\n",
      "Batch 3820/4897, Loss: 0.6927300691604614\n",
      "Batch 3830/4897, Loss: 0.6933495402336121\n",
      "Batch 3840/4897, Loss: 0.6922642588615417\n",
      "Batch 3850/4897, Loss: 0.6917802095413208\n",
      "Batch 3860/4897, Loss: 0.6903975605964661\n",
      "Batch 3870/4897, Loss: 0.6885977387428284\n",
      "Batch 3880/4897, Loss: 0.6900807023048401\n",
      "Batch 3890/4897, Loss: 0.6917035579681396\n",
      "Batch 3900/4897, Loss: 0.6942697167396545\n",
      "Batch 3910/4897, Loss: 0.6876715421676636\n",
      "Batch 3920/4897, Loss: 0.6934117674827576\n",
      "Batch 3930/4897, Loss: 0.6891846656799316\n",
      "Batch 3940/4897, Loss: 0.6927494406700134\n",
      "Batch 3950/4897, Loss: 0.6993937492370605\n",
      "Batch 3960/4897, Loss: 0.694286584854126\n",
      "Batch 3970/4897, Loss: 0.694506824016571\n",
      "Batch 3980/4897, Loss: 0.6906697750091553\n",
      "Batch 3990/4897, Loss: 0.6930452585220337\n",
      "Batch 4000/4897, Loss: 0.6949117183685303\n",
      "Batch 4010/4897, Loss: 0.6937738060951233\n",
      "Batch 4020/4897, Loss: 0.6955146193504333\n",
      "Batch 4030/4897, Loss: 0.69432133436203\n",
      "Batch 4040/4897, Loss: 0.6938965320587158\n",
      "Batch 4050/4897, Loss: 0.6933801174163818\n",
      "Batch 4060/4897, Loss: 0.6926618218421936\n",
      "Batch 4070/4897, Loss: 0.6915722489356995\n",
      "Batch 4080/4897, Loss: 0.6930482387542725\n",
      "Batch 4090/4897, Loss: 0.6953856945037842\n",
      "Batch 4100/4897, Loss: 0.6915474534034729\n",
      "Batch 4110/4897, Loss: 0.6922315359115601\n",
      "Batch 4120/4897, Loss: 0.6952047944068909\n",
      "Batch 4130/4897, Loss: 0.6963807344436646\n",
      "Batch 4140/4897, Loss: 0.691979169845581\n",
      "Batch 4150/4897, Loss: 0.6928578615188599\n",
      "Batch 4160/4897, Loss: 0.6896060705184937\n",
      "Batch 4170/4897, Loss: 0.6952338218688965\n",
      "Batch 4180/4897, Loss: 0.691991925239563\n",
      "Batch 4190/4897, Loss: 0.6925821304321289\n",
      "Batch 4200/4897, Loss: 0.6930927038192749\n",
      "Batch 4210/4897, Loss: 0.6920915246009827\n",
      "Batch 4220/4897, Loss: 0.6921252012252808\n",
      "Batch 4230/4897, Loss: 0.6927027702331543\n",
      "Batch 4240/4897, Loss: 0.6931658983230591\n",
      "Batch 4250/4897, Loss: 0.6921019554138184\n",
      "Batch 4260/4897, Loss: 0.6939482688903809\n",
      "Batch 4270/4897, Loss: 0.692602276802063\n",
      "Batch 4280/4897, Loss: 0.6945889592170715\n",
      "Batch 4290/4897, Loss: 0.6934628486633301\n",
      "Batch 4300/4897, Loss: 0.6941673159599304\n",
      "Batch 4310/4897, Loss: 0.69416344165802\n",
      "Batch 4320/4897, Loss: 0.6930303573608398\n",
      "Batch 4330/4897, Loss: 0.6912480592727661\n",
      "Batch 4340/4897, Loss: 0.6914953589439392\n",
      "Batch 4350/4897, Loss: 0.6917224526405334\n",
      "Batch 4360/4897, Loss: 0.6929051876068115\n",
      "Batch 4370/4897, Loss: 0.6910725235939026\n",
      "Batch 4380/4897, Loss: 0.6939780116081238\n",
      "Batch 4390/4897, Loss: 0.6908837556838989\n",
      "Batch 4400/4897, Loss: 0.68889319896698\n",
      "Batch 4410/4897, Loss: 0.6837328672409058\n",
      "Batch 4420/4897, Loss: 0.6925433874130249\n",
      "Batch 4430/4897, Loss: 0.6974833011627197\n",
      "Batch 4440/4897, Loss: 0.6913254857063293\n",
      "Batch 4450/4897, Loss: 0.6925181150436401\n",
      "Batch 4460/4897, Loss: 0.6947872638702393\n",
      "Batch 4470/4897, Loss: 0.6934992671012878\n",
      "Batch 4480/4897, Loss: 0.6929786205291748\n",
      "Batch 4490/4897, Loss: 0.6861186027526855\n",
      "Batch 4500/4897, Loss: 0.6880039572715759\n",
      "Batch 4510/4897, Loss: 0.6875695586204529\n",
      "Batch 4520/4897, Loss: 0.6994372606277466\n",
      "Batch 4530/4897, Loss: 0.6948402523994446\n",
      "Batch 4540/4897, Loss: 0.7017219662666321\n",
      "Batch 4550/4897, Loss: 0.6926825642585754\n",
      "Batch 4560/4897, Loss: 0.6885607838630676\n",
      "Batch 4570/4897, Loss: 0.6934666037559509\n",
      "Batch 4580/4897, Loss: 0.6908923387527466\n",
      "Batch 4590/4897, Loss: 0.6868582963943481\n",
      "Batch 4600/4897, Loss: 0.6882016658782959\n",
      "Batch 4610/4897, Loss: 0.6957889199256897\n",
      "Batch 4620/4897, Loss: 0.6916623115539551\n",
      "Batch 4630/4897, Loss: 0.6926892399787903\n",
      "Batch 4640/4897, Loss: 0.6901191473007202\n",
      "Batch 4650/4897, Loss: 0.6933537721633911\n",
      "Batch 4660/4897, Loss: 0.6978113651275635\n",
      "Batch 4670/4897, Loss: 0.6936285495758057\n",
      "Batch 4680/4897, Loss: 0.6914575695991516\n",
      "Batch 4690/4897, Loss: 0.693298876285553\n",
      "Batch 4700/4897, Loss: 0.6938976645469666\n",
      "Batch 4710/4897, Loss: 0.6922730803489685\n",
      "Batch 4720/4897, Loss: 0.6919078826904297\n",
      "Batch 4730/4897, Loss: 0.6937469244003296\n",
      "Batch 4740/4897, Loss: 0.6926490068435669\n",
      "Batch 4750/4897, Loss: 0.6937704682350159\n",
      "Batch 4760/4897, Loss: 0.6925816535949707\n",
      "Batch 4770/4897, Loss: 0.6924601793289185\n",
      "Batch 4780/4897, Loss: 0.6930123567581177\n",
      "Batch 4790/4897, Loss: 0.6944818496704102\n",
      "Batch 4800/4897, Loss: 0.693530797958374\n",
      "Batch 4810/4897, Loss: 0.6947115063667297\n",
      "Batch 4820/4897, Loss: 0.6931134462356567\n",
      "Batch 4830/4897, Loss: 0.6930820941925049\n",
      "Batch 4840/4897, Loss: 0.6928884387016296\n",
      "Batch 4850/4897, Loss: 0.6939476728439331\n",
      "Batch 4860/4897, Loss: 0.6931339502334595\n",
      "Batch 4870/4897, Loss: 0.6935778260231018\n",
      "Batch 4880/4897, Loss: 0.6930077075958252\n",
      "Batch 4890/4897, Loss: 0.6934215426445007\n",
      "Epoch 1/2, Loss: 0.6958504319190979\n",
      "Batch 0/4897, Loss: 0.69228595495224\n",
      "Batch 10/4897, Loss: 0.6934599280357361\n",
      "Batch 20/4897, Loss: 0.6916857957839966\n",
      "Batch 30/4897, Loss: 0.6908759474754333\n",
      "Batch 40/4897, Loss: 0.6938362121582031\n",
      "Batch 50/4897, Loss: 0.693793535232544\n",
      "Batch 60/4897, Loss: 0.6915256977081299\n",
      "Batch 70/4897, Loss: 0.6908363103866577\n",
      "Batch 80/4897, Loss: 0.691265344619751\n",
      "Batch 90/4897, Loss: 0.6946466565132141\n",
      "Batch 100/4897, Loss: 0.6944421529769897\n",
      "Batch 110/4897, Loss: 0.6927275657653809\n",
      "Batch 120/4897, Loss: 0.6949220299720764\n",
      "Batch 130/4897, Loss: 0.6925002932548523\n",
      "Batch 140/4897, Loss: 0.6916214227676392\n",
      "Batch 150/4897, Loss: 0.6937424540519714\n",
      "Batch 160/4897, Loss: 0.6933746933937073\n",
      "Batch 170/4897, Loss: 0.6927390098571777\n",
      "Batch 180/4897, Loss: 0.6919935345649719\n",
      "Batch 190/4897, Loss: 0.6928314566612244\n",
      "Batch 200/4897, Loss: 0.691020667552948\n",
      "Batch 210/4897, Loss: 0.6919220685958862\n",
      "Batch 220/4897, Loss: 0.6971185207366943\n",
      "Batch 230/4897, Loss: 0.687358021736145\n",
      "Batch 240/4897, Loss: 0.6934080123901367\n",
      "Batch 250/4897, Loss: 0.6976624131202698\n",
      "Batch 260/4897, Loss: 0.6904529929161072\n",
      "Batch 270/4897, Loss: 0.6974229216575623\n",
      "Batch 280/4897, Loss: 0.6903434991836548\n",
      "Batch 290/4897, Loss: 0.6875868439674377\n",
      "Batch 300/4897, Loss: 0.6920387148857117\n",
      "Batch 310/4897, Loss: 0.6856086850166321\n",
      "Batch 320/4897, Loss: 0.6921179294586182\n",
      "Batch 330/4897, Loss: 0.6899802684783936\n",
      "Batch 340/4897, Loss: 0.6882359385490417\n",
      "Batch 350/4897, Loss: 0.6972070336341858\n",
      "Batch 360/4897, Loss: 0.6878291368484497\n",
      "Batch 370/4897, Loss: 0.6919320821762085\n",
      "Batch 380/4897, Loss: 0.6945721507072449\n",
      "Batch 390/4897, Loss: 0.6833716034889221\n",
      "Batch 400/4897, Loss: 0.6858415603637695\n",
      "Batch 410/4897, Loss: 0.6849094033241272\n",
      "Batch 420/4897, Loss: 0.6826956868171692\n",
      "Batch 430/4897, Loss: 0.6936648488044739\n",
      "Batch 440/4897, Loss: 0.6937713623046875\n",
      "Batch 450/4897, Loss: 0.7009434700012207\n",
      "Batch 460/4897, Loss: 0.6944727301597595\n",
      "Batch 470/4897, Loss: 0.6902267932891846\n",
      "Batch 480/4897, Loss: 0.6949588060379028\n",
      "Batch 490/4897, Loss: 0.6906768679618835\n",
      "Batch 500/4897, Loss: 0.6872076988220215\n",
      "Batch 510/4897, Loss: 0.6904579401016235\n",
      "Batch 520/4897, Loss: 0.700127363204956\n",
      "Batch 530/4897, Loss: 0.6974709630012512\n",
      "Batch 540/4897, Loss: 0.694189190864563\n",
      "Batch 550/4897, Loss: 0.6936824321746826\n",
      "Batch 560/4897, Loss: 0.6999819874763489\n",
      "Batch 570/4897, Loss: 0.6905897259712219\n",
      "Batch 580/4897, Loss: 0.6962862014770508\n",
      "Batch 590/4897, Loss: 0.6930058598518372\n",
      "Batch 600/4897, Loss: 0.6912165880203247\n",
      "Batch 610/4897, Loss: 0.691922664642334\n",
      "Batch 620/4897, Loss: 0.6851401329040527\n",
      "Batch 630/4897, Loss: 0.6938947439193726\n",
      "Batch 640/4897, Loss: 0.7007070779800415\n",
      "Batch 650/4897, Loss: 0.693647563457489\n",
      "Batch 660/4897, Loss: 0.6937885880470276\n",
      "Batch 670/4897, Loss: 0.6926698684692383\n",
      "Batch 680/4897, Loss: 0.6963446736335754\n",
      "Batch 690/4897, Loss: 0.6955100297927856\n",
      "Batch 700/4897, Loss: 0.6883136630058289\n",
      "Batch 710/4897, Loss: 0.6890872120857239\n",
      "Batch 720/4897, Loss: 0.6853060126304626\n",
      "Batch 730/4897, Loss: 0.6955710649490356\n",
      "Batch 740/4897, Loss: 0.7017002105712891\n",
      "Batch 750/4897, Loss: 0.6849924325942993\n",
      "Batch 760/4897, Loss: 0.6837055087089539\n",
      "Batch 770/4897, Loss: 0.6929615139961243\n",
      "Batch 780/4897, Loss: 0.692145049571991\n",
      "Batch 790/4897, Loss: 0.6954337954521179\n",
      "Batch 800/4897, Loss: 0.6905373334884644\n",
      "Batch 810/4897, Loss: 0.6927016973495483\n",
      "Batch 820/4897, Loss: 0.6955281496047974\n",
      "Batch 830/4897, Loss: 0.6903491616249084\n",
      "Batch 840/4897, Loss: 0.6912046670913696\n",
      "Batch 850/4897, Loss: 0.6818214654922485\n",
      "Batch 860/4897, Loss: 0.6936074495315552\n",
      "Batch 870/4897, Loss: 0.6933117508888245\n",
      "Batch 880/4897, Loss: 0.6998414397239685\n",
      "Batch 890/4897, Loss: 0.6967847347259521\n",
      "Batch 900/4897, Loss: 0.7066717743873596\n",
      "Batch 910/4897, Loss: 0.6893318891525269\n",
      "Batch 920/4897, Loss: 0.689815104007721\n",
      "Batch 930/4897, Loss: 0.6993511319160461\n",
      "Batch 940/4897, Loss: 0.6851876378059387\n",
      "Batch 950/4897, Loss: 0.6888014078140259\n",
      "Batch 960/4897, Loss: 0.6897786259651184\n",
      "Batch 970/4897, Loss: 0.694539487361908\n",
      "Batch 980/4897, Loss: 0.7031679153442383\n",
      "Batch 990/4897, Loss: 0.6926680207252502\n",
      "Batch 1000/4897, Loss: 0.6923986077308655\n",
      "Batch 1010/4897, Loss: 0.693917989730835\n",
      "Batch 1020/4897, Loss: 0.6849741339683533\n",
      "Batch 1030/4897, Loss: 0.688647449016571\n",
      "Batch 1040/4897, Loss: 0.6937958598136902\n",
      "Batch 1050/4897, Loss: 0.6856278777122498\n",
      "Batch 1060/4897, Loss: 0.6804218292236328\n",
      "Batch 1070/4897, Loss: 0.678135871887207\n",
      "Batch 1080/4897, Loss: 0.6848825812339783\n",
      "Batch 1090/4897, Loss: 0.6833808422088623\n",
      "Batch 1100/4897, Loss: 0.6949219703674316\n",
      "Batch 1110/4897, Loss: 0.6848251819610596\n",
      "Batch 1120/4897, Loss: 0.6916007995605469\n",
      "Batch 1130/4897, Loss: 0.6896966099739075\n",
      "Batch 1140/4897, Loss: 0.7012048959732056\n",
      "Batch 1150/4897, Loss: 0.6939862966537476\n",
      "Batch 1160/4897, Loss: 0.700516939163208\n",
      "Batch 1170/4897, Loss: 0.6852587461471558\n",
      "Batch 1180/4897, Loss: 0.6905534863471985\n",
      "Batch 1190/4897, Loss: 0.6993597745895386\n",
      "Batch 1200/4897, Loss: 0.6887698173522949\n",
      "Batch 1210/4897, Loss: 0.6836576461791992\n",
      "Batch 1220/4897, Loss: 0.6973550319671631\n",
      "Batch 1230/4897, Loss: 0.6880306601524353\n",
      "Batch 1240/4897, Loss: 0.6912102699279785\n",
      "Batch 1250/4897, Loss: 0.6763441562652588\n",
      "Batch 1260/4897, Loss: 0.6916100382804871\n",
      "Batch 1270/4897, Loss: 0.6948034167289734\n",
      "Batch 1280/4897, Loss: 0.6961395740509033\n",
      "Batch 1290/4897, Loss: 0.6884142160415649\n",
      "Batch 1300/4897, Loss: 0.6892732977867126\n",
      "Batch 1310/4897, Loss: 0.6929922103881836\n",
      "Batch 1320/4897, Loss: 0.6768615245819092\n",
      "Batch 1330/4897, Loss: 0.6842175126075745\n",
      "Batch 1340/4897, Loss: 0.698108434677124\n",
      "Batch 1350/4897, Loss: 0.6869553327560425\n",
      "Batch 1360/4897, Loss: 0.6867756247520447\n",
      "Batch 1370/4897, Loss: 0.6844240427017212\n",
      "Batch 1380/4897, Loss: 0.6942037343978882\n",
      "Batch 1390/4897, Loss: 0.6863119602203369\n",
      "Batch 1400/4897, Loss: 0.6873068809509277\n",
      "Batch 1410/4897, Loss: 0.6929030418395996\n",
      "Batch 1420/4897, Loss: 0.702130138874054\n",
      "Batch 1430/4897, Loss: 0.6788252592086792\n",
      "Batch 1440/4897, Loss: 0.6959726214408875\n",
      "Batch 1450/4897, Loss: 0.6997593641281128\n",
      "Batch 1460/4897, Loss: 0.6987975835800171\n",
      "Batch 1470/4897, Loss: 0.6939325332641602\n",
      "Batch 1480/4897, Loss: 0.6917405128479004\n",
      "Batch 1490/4897, Loss: 0.7109054327011108\n",
      "Batch 1500/4897, Loss: 0.6859475374221802\n",
      "Batch 1510/4897, Loss: 0.687384307384491\n",
      "Batch 1520/4897, Loss: 0.6891419887542725\n",
      "Batch 1530/4897, Loss: 0.6843471527099609\n",
      "Batch 1540/4897, Loss: 0.6980352997779846\n",
      "Batch 1550/4897, Loss: 0.6933913826942444\n",
      "Batch 1560/4897, Loss: 0.7028676867485046\n",
      "Batch 1570/4897, Loss: 0.6864886283874512\n",
      "Batch 1580/4897, Loss: 0.6846305727958679\n",
      "Batch 1590/4897, Loss: 0.6823086142539978\n",
      "Batch 1600/4897, Loss: 0.6848546266555786\n",
      "Batch 1610/4897, Loss: 0.6960474252700806\n",
      "Batch 1620/4897, Loss: 0.6820687651634216\n",
      "Batch 1630/4897, Loss: 0.686083972454071\n",
      "Batch 1640/4897, Loss: 0.6945478320121765\n",
      "Batch 1650/4897, Loss: 0.70339435338974\n",
      "Batch 1660/4897, Loss: 0.6768922209739685\n",
      "Batch 1670/4897, Loss: 0.6947035193443298\n",
      "Batch 1680/4897, Loss: 0.706013023853302\n",
      "Batch 1690/4897, Loss: 0.6904131174087524\n",
      "Batch 1700/4897, Loss: 0.6888007521629333\n",
      "Batch 1710/4897, Loss: 0.6945091485977173\n",
      "Batch 1720/4897, Loss: 0.6883852481842041\n",
      "Batch 1730/4897, Loss: 0.697647213935852\n",
      "Batch 1740/4897, Loss: 0.6944844126701355\n",
      "Batch 1750/4897, Loss: 0.6822898983955383\n",
      "Batch 1760/4897, Loss: 0.6973520517349243\n",
      "Batch 1770/4897, Loss: 0.699091374874115\n",
      "Batch 1780/4897, Loss: 0.6901831030845642\n",
      "Batch 1790/4897, Loss: 0.6994691491127014\n",
      "Batch 1800/4897, Loss: 0.700376570224762\n",
      "Batch 1810/4897, Loss: 0.6921452283859253\n",
      "Batch 1820/4897, Loss: 0.6912486553192139\n",
      "Batch 1830/4897, Loss: 0.6747344136238098\n",
      "Batch 1840/4897, Loss: 0.7035873532295227\n",
      "Batch 1850/4897, Loss: 0.6970269680023193\n",
      "Batch 1860/4897, Loss: 0.6826797127723694\n",
      "Batch 1870/4897, Loss: 0.685489296913147\n",
      "Batch 1880/4897, Loss: 0.6735226511955261\n",
      "Batch 1890/4897, Loss: 0.7019405961036682\n",
      "Batch 1900/4897, Loss: 0.6879774332046509\n",
      "Batch 1910/4897, Loss: 0.6867791414260864\n",
      "Batch 1920/4897, Loss: 0.699097752571106\n",
      "Batch 1930/4897, Loss: 0.6969166994094849\n",
      "Batch 1940/4897, Loss: 0.6912331581115723\n",
      "Batch 1950/4897, Loss: 0.6961130499839783\n",
      "Batch 1960/4897, Loss: 0.6929844617843628\n",
      "Batch 1970/4897, Loss: 0.6994697451591492\n",
      "Batch 1980/4897, Loss: 0.6789345145225525\n",
      "Batch 1990/4897, Loss: 0.7010253071784973\n",
      "Batch 2000/4897, Loss: 0.6821714639663696\n",
      "Batch 2010/4897, Loss: 0.6828606724739075\n",
      "Batch 2020/4897, Loss: 0.6743080019950867\n",
      "Batch 2030/4897, Loss: 0.6881975531578064\n",
      "Batch 2040/4897, Loss: 0.6927875876426697\n",
      "Batch 2050/4897, Loss: 0.6807880997657776\n",
      "Batch 2060/4897, Loss: 0.7008427977561951\n",
      "Batch 2070/4897, Loss: 0.6964225769042969\n",
      "Batch 2080/4897, Loss: 0.6938576698303223\n",
      "Batch 2090/4897, Loss: 0.6906797885894775\n",
      "Batch 2100/4897, Loss: 0.6914963126182556\n",
      "Batch 2110/4897, Loss: 0.6996243000030518\n",
      "Batch 2120/4897, Loss: 0.6862537860870361\n",
      "Batch 2130/4897, Loss: 0.7092587947845459\n",
      "Batch 2140/4897, Loss: 0.694919228553772\n",
      "Batch 2150/4897, Loss: 0.6929985284805298\n",
      "Batch 2160/4897, Loss: 0.7038876414299011\n",
      "Batch 2170/4897, Loss: 0.6893168687820435\n",
      "Batch 2180/4897, Loss: 0.696374773979187\n",
      "Batch 2190/4897, Loss: 0.6963882446289062\n",
      "Batch 2200/4897, Loss: 0.6980064511299133\n",
      "Batch 2210/4897, Loss: 0.6809808611869812\n",
      "Batch 2220/4897, Loss: 0.6989713907241821\n",
      "Batch 2230/4897, Loss: 0.7004924416542053\n",
      "Batch 2240/4897, Loss: 0.6818975806236267\n",
      "Batch 2250/4897, Loss: 0.6951179504394531\n",
      "Batch 2260/4897, Loss: 0.7007503509521484\n",
      "Batch 2270/4897, Loss: 0.6977244019508362\n",
      "Batch 2280/4897, Loss: 0.7043746709823608\n",
      "Batch 2290/4897, Loss: 0.6904492974281311\n",
      "Batch 2300/4897, Loss: 0.6852687001228333\n",
      "Batch 2310/4897, Loss: 0.6827729344367981\n",
      "Batch 2320/4897, Loss: 0.6976687908172607\n",
      "Batch 2330/4897, Loss: 0.6938486099243164\n",
      "Batch 2340/4897, Loss: 0.6893677711486816\n",
      "Batch 2350/4897, Loss: 0.6897298693656921\n",
      "Batch 2360/4897, Loss: 0.6961120367050171\n",
      "Batch 2370/4897, Loss: 0.6901365518569946\n",
      "Batch 2380/4897, Loss: 0.6916717290878296\n",
      "Batch 2390/4897, Loss: 0.6858455538749695\n",
      "Batch 2400/4897, Loss: 0.6753830313682556\n",
      "Batch 2410/4897, Loss: 0.6916834115982056\n",
      "Batch 2420/4897, Loss: 0.6913976669311523\n",
      "Batch 2430/4897, Loss: 0.68878173828125\n",
      "Batch 2440/4897, Loss: 0.7087210416793823\n",
      "Batch 2450/4897, Loss: 0.690911054611206\n",
      "Batch 2460/4897, Loss: 0.6866162419319153\n",
      "Batch 2470/4897, Loss: 0.6932869553565979\n",
      "Batch 2480/4897, Loss: 0.6814063787460327\n",
      "Batch 2490/4897, Loss: 0.6780069470405579\n",
      "Batch 2500/4897, Loss: 0.6958571076393127\n",
      "Batch 2510/4897, Loss: 0.6918070316314697\n",
      "Batch 2520/4897, Loss: 0.686668336391449\n",
      "Batch 2530/4897, Loss: 0.6820570230484009\n",
      "Batch 2540/4897, Loss: 0.6857273578643799\n",
      "Batch 2550/4897, Loss: 0.6987396478652954\n",
      "Batch 2560/4897, Loss: 0.6819201707839966\n",
      "Batch 2570/4897, Loss: 0.6806483864784241\n",
      "Batch 2580/4897, Loss: 0.6965180039405823\n",
      "Batch 2590/4897, Loss: 0.7000350952148438\n",
      "Batch 2600/4897, Loss: 0.6783890128135681\n",
      "Batch 2610/4897, Loss: 0.7076826691627502\n",
      "Batch 2620/4897, Loss: 0.6825680732727051\n",
      "Batch 2630/4897, Loss: 0.6826406717300415\n",
      "Batch 2640/4897, Loss: 0.6782328486442566\n",
      "Batch 2650/4897, Loss: 0.6791384220123291\n",
      "Batch 2660/4897, Loss: 0.6906032562255859\n",
      "Batch 2670/4897, Loss: 0.690959632396698\n",
      "Batch 2680/4897, Loss: 0.6799219846725464\n",
      "Batch 2690/4897, Loss: 0.6879690885543823\n",
      "Batch 2700/4897, Loss: 0.7023007869720459\n",
      "Batch 2710/4897, Loss: 0.6832372546195984\n",
      "Batch 2720/4897, Loss: 0.6847894787788391\n",
      "Batch 2730/4897, Loss: 0.6881434917449951\n",
      "Batch 2740/4897, Loss: 0.7056270241737366\n",
      "Batch 2750/4897, Loss: 0.7002233266830444\n",
      "Batch 2760/4897, Loss: 0.6995580196380615\n",
      "Batch 2770/4897, Loss: 0.7042543888092041\n",
      "Batch 2780/4897, Loss: 0.6888646483421326\n",
      "Batch 2790/4897, Loss: 0.6938498020172119\n",
      "Batch 2800/4897, Loss: 0.6923834085464478\n",
      "Batch 2810/4897, Loss: 0.6910983920097351\n",
      "Batch 2820/4897, Loss: 0.6909126043319702\n",
      "Batch 2830/4897, Loss: 0.6929358243942261\n",
      "Batch 2840/4897, Loss: 0.6910184621810913\n",
      "Batch 2850/4897, Loss: 0.7089883089065552\n",
      "Batch 2860/4897, Loss: 0.6953209042549133\n",
      "Batch 2870/4897, Loss: 0.7010758519172668\n",
      "Batch 2880/4897, Loss: 0.6976956129074097\n",
      "Batch 2890/4897, Loss: 0.6958025693893433\n",
      "Batch 2900/4897, Loss: 0.6843538284301758\n",
      "Batch 2910/4897, Loss: 0.6892556548118591\n",
      "Batch 2920/4897, Loss: 0.6949198842048645\n",
      "Batch 2930/4897, Loss: 0.6889165639877319\n",
      "Batch 2940/4897, Loss: 0.6761370301246643\n",
      "Batch 2950/4897, Loss: 0.684753954410553\n",
      "Batch 2960/4897, Loss: 0.6936541199684143\n",
      "Batch 2970/4897, Loss: 0.6882602572441101\n",
      "Batch 2980/4897, Loss: 0.6977611780166626\n",
      "Batch 2990/4897, Loss: 0.7169144749641418\n",
      "Batch 3000/4897, Loss: 0.7061193585395813\n",
      "Batch 3010/4897, Loss: 0.701595664024353\n",
      "Batch 3020/4897, Loss: 0.686298668384552\n",
      "Batch 3030/4897, Loss: 0.7043342590332031\n",
      "Batch 3040/4897, Loss: 0.6990634202957153\n",
      "Batch 3050/4897, Loss: 0.709438145160675\n",
      "Batch 3060/4897, Loss: 0.6991598010063171\n",
      "Batch 3070/4897, Loss: 0.6974679231643677\n",
      "Batch 3080/4897, Loss: 0.6931918263435364\n",
      "Batch 3090/4897, Loss: 0.6791835427284241\n",
      "Batch 3100/4897, Loss: 0.6887527704238892\n",
      "Batch 3110/4897, Loss: 0.693135678768158\n",
      "Batch 3120/4897, Loss: 0.669772207736969\n",
      "Batch 3130/4897, Loss: 0.6906836032867432\n",
      "Batch 3140/4897, Loss: 0.6908777356147766\n",
      "Batch 3150/4897, Loss: 0.6969187259674072\n",
      "Batch 3160/4897, Loss: 0.671922504901886\n",
      "Batch 3170/4897, Loss: 0.6870325803756714\n",
      "Batch 3180/4897, Loss: 0.6853166818618774\n",
      "Batch 3190/4897, Loss: 0.7005469799041748\n",
      "Batch 3200/4897, Loss: 0.6955528855323792\n",
      "Batch 3210/4897, Loss: 0.6834388971328735\n",
      "Batch 3220/4897, Loss: 0.6895459294319153\n",
      "Batch 3230/4897, Loss: 0.6912833452224731\n",
      "Batch 3240/4897, Loss: 0.686953604221344\n",
      "Batch 3250/4897, Loss: 0.6911705136299133\n",
      "Batch 3260/4897, Loss: 0.6896272301673889\n",
      "Batch 3270/4897, Loss: 0.6927917003631592\n",
      "Batch 3280/4897, Loss: 0.6903671026229858\n",
      "Batch 3290/4897, Loss: 0.6858765482902527\n",
      "Batch 3300/4897, Loss: 0.7060402631759644\n",
      "Batch 3310/4897, Loss: 0.6896834969520569\n",
      "Batch 3320/4897, Loss: 0.7061712145805359\n",
      "Batch 3330/4897, Loss: 0.6975408792495728\n",
      "Batch 3340/4897, Loss: 0.6935604810714722\n",
      "Batch 3350/4897, Loss: 0.6859303116798401\n",
      "Batch 3360/4897, Loss: 0.6805315017700195\n",
      "Batch 3370/4897, Loss: 0.6937665939331055\n",
      "Batch 3380/4897, Loss: 0.6928538084030151\n",
      "Batch 3390/4897, Loss: 0.7080274820327759\n",
      "Batch 3400/4897, Loss: 0.6946340799331665\n",
      "Batch 3410/4897, Loss: 0.7137759327888489\n",
      "Batch 3420/4897, Loss: 0.6921390295028687\n",
      "Batch 3430/4897, Loss: 0.6961209774017334\n",
      "Batch 3440/4897, Loss: 0.6942557096481323\n",
      "Batch 3450/4897, Loss: 0.696238100528717\n",
      "Batch 3460/4897, Loss: 0.6885981559753418\n",
      "Batch 3470/4897, Loss: 0.6997402906417847\n",
      "Batch 3480/4897, Loss: 0.7067654132843018\n",
      "Batch 3490/4897, Loss: 0.698578953742981\n",
      "Batch 3500/4897, Loss: 0.6877740025520325\n",
      "Batch 3510/4897, Loss: 0.6816889047622681\n",
      "Batch 3520/4897, Loss: 0.6925480365753174\n",
      "Batch 3530/4897, Loss: 0.6979067325592041\n",
      "Batch 3540/4897, Loss: 0.6943032741546631\n",
      "Batch 3550/4897, Loss: 0.6941237449645996\n",
      "Batch 3560/4897, Loss: 0.6847476363182068\n",
      "Batch 3570/4897, Loss: 0.6913983821868896\n",
      "Batch 3580/4897, Loss: 0.6948381662368774\n",
      "Batch 3590/4897, Loss: 0.6944802403450012\n",
      "Batch 3600/4897, Loss: 0.7064722776412964\n",
      "Batch 3610/4897, Loss: 0.6820348501205444\n",
      "Batch 3620/4897, Loss: 0.6920825242996216\n",
      "Batch 3630/4897, Loss: 0.6961642503738403\n",
      "Batch 3640/4897, Loss: 0.714476466178894\n",
      "Batch 3650/4897, Loss: 0.6926528811454773\n",
      "Batch 3660/4897, Loss: 0.6840664148330688\n",
      "Batch 3670/4897, Loss: 0.6782373785972595\n",
      "Batch 3680/4897, Loss: 0.7020204067230225\n",
      "Batch 3690/4897, Loss: 0.7025604248046875\n",
      "Batch 3700/4897, Loss: 0.6983671188354492\n",
      "Batch 3710/4897, Loss: 0.6822783946990967\n",
      "Batch 3720/4897, Loss: 0.6828803420066833\n",
      "Batch 3730/4897, Loss: 0.7033287882804871\n",
      "Batch 3740/4897, Loss: 0.7023200392723083\n",
      "Batch 3750/4897, Loss: 0.6813210248947144\n",
      "Batch 3760/4897, Loss: 0.6860838532447815\n",
      "Batch 3770/4897, Loss: 0.6994301080703735\n",
      "Batch 3780/4897, Loss: 0.6909694075584412\n",
      "Batch 3790/4897, Loss: 0.6927149891853333\n",
      "Batch 3800/4897, Loss: 0.6870676875114441\n",
      "Batch 3810/4897, Loss: 0.6918419003486633\n",
      "Batch 3820/4897, Loss: 0.70237135887146\n",
      "Batch 3830/4897, Loss: 0.6939578056335449\n",
      "Batch 3840/4897, Loss: 0.6823397874832153\n",
      "Batch 3850/4897, Loss: 0.6937728524208069\n",
      "Batch 3860/4897, Loss: 0.6904955506324768\n",
      "Batch 3870/4897, Loss: 0.6875346899032593\n",
      "Batch 3880/4897, Loss: 0.6738715767860413\n",
      "Batch 3890/4897, Loss: 0.7042649388313293\n",
      "Batch 3900/4897, Loss: 0.683829128742218\n",
      "Batch 3910/4897, Loss: 0.7064433097839355\n",
      "Batch 3920/4897, Loss: 0.6899981498718262\n",
      "Batch 3930/4897, Loss: 0.6994725465774536\n",
      "Batch 3940/4897, Loss: 0.677861750125885\n",
      "Batch 3950/4897, Loss: 0.71211838722229\n",
      "Batch 3960/4897, Loss: 0.6810768246650696\n",
      "Batch 3970/4897, Loss: 0.6958558559417725\n",
      "Batch 3980/4897, Loss: 0.6899827718734741\n",
      "Batch 3990/4897, Loss: 0.7060574293136597\n",
      "Batch 4000/4897, Loss: 0.6920522451400757\n",
      "Batch 4010/4897, Loss: 0.6878184676170349\n",
      "Batch 4020/4897, Loss: 0.6986979842185974\n",
      "Batch 4030/4897, Loss: 0.6959635615348816\n",
      "Batch 4040/4897, Loss: 0.6878311038017273\n",
      "Batch 4050/4897, Loss: 0.6854860186576843\n",
      "Batch 4060/4897, Loss: 0.6849690675735474\n",
      "Batch 4070/4897, Loss: 0.7072580456733704\n",
      "Batch 4080/4897, Loss: 0.6852724552154541\n",
      "Batch 4090/4897, Loss: 0.7003922462463379\n",
      "Batch 4100/4897, Loss: 0.6922776699066162\n",
      "Batch 4110/4897, Loss: 0.711448073387146\n",
      "Batch 4120/4897, Loss: 0.6909027099609375\n",
      "Batch 4130/4897, Loss: 0.7071678638458252\n",
      "Batch 4140/4897, Loss: 0.6814385652542114\n",
      "Batch 4150/4897, Loss: 0.6909294128417969\n",
      "Batch 4160/4897, Loss: 0.7033720016479492\n",
      "Batch 4170/4897, Loss: 0.6910808086395264\n",
      "Batch 4180/4897, Loss: 0.6911852359771729\n",
      "Batch 4190/4897, Loss: 0.6913082599639893\n",
      "Batch 4200/4897, Loss: 0.69008469581604\n",
      "Batch 4210/4897, Loss: 0.6857994794845581\n",
      "Batch 4220/4897, Loss: 0.6845994591712952\n",
      "Batch 4230/4897, Loss: 0.6994180083274841\n",
      "Batch 4240/4897, Loss: 0.690529465675354\n",
      "Batch 4250/4897, Loss: 0.690386950969696\n",
      "Batch 4260/4897, Loss: 0.6952030062675476\n",
      "Batch 4270/4897, Loss: 0.683644711971283\n",
      "Batch 4280/4897, Loss: 0.7036716341972351\n",
      "Batch 4290/4897, Loss: 0.6903153657913208\n",
      "Batch 4300/4897, Loss: 0.6700296401977539\n",
      "Batch 4310/4897, Loss: 0.6840019226074219\n",
      "Batch 4320/4897, Loss: 0.7062851190567017\n",
      "Batch 4330/4897, Loss: 0.6754209399223328\n",
      "Batch 4340/4897, Loss: 0.7082552909851074\n",
      "Batch 4350/4897, Loss: 0.6963749527931213\n",
      "Batch 4360/4897, Loss: 0.6731945872306824\n",
      "Batch 4370/4897, Loss: 0.6996195316314697\n",
      "Batch 4380/4897, Loss: 0.7091085314750671\n",
      "Batch 4390/4897, Loss: 0.6972717642784119\n",
      "Batch 4400/4897, Loss: 0.6884340047836304\n",
      "Batch 4410/4897, Loss: 0.6911948919296265\n",
      "Batch 4420/4897, Loss: 0.6897072196006775\n",
      "Batch 4430/4897, Loss: 0.686462938785553\n",
      "Batch 4440/4897, Loss: 0.7067369818687439\n",
      "Batch 4450/4897, Loss: 0.6970394253730774\n",
      "Batch 4460/4897, Loss: 0.702908456325531\n",
      "Batch 4470/4897, Loss: 0.6704360246658325\n",
      "Batch 4480/4897, Loss: 0.699828028678894\n",
      "Batch 4490/4897, Loss: 0.6959635019302368\n",
      "Batch 4500/4897, Loss: 0.6860225200653076\n",
      "Batch 4510/4897, Loss: 0.691530168056488\n",
      "Batch 4520/4897, Loss: 0.6944062113761902\n",
      "Batch 4530/4897, Loss: 0.6990005970001221\n",
      "Batch 4540/4897, Loss: 0.697507381439209\n",
      "Batch 4550/4897, Loss: 0.6914478540420532\n",
      "Batch 4560/4897, Loss: 0.683687686920166\n",
      "Batch 4570/4897, Loss: 0.6888646483421326\n",
      "Batch 4580/4897, Loss: 0.6913945078849792\n",
      "Batch 4590/4897, Loss: 0.6846327781677246\n",
      "Batch 4600/4897, Loss: 0.7048312425613403\n",
      "Batch 4610/4897, Loss: 0.6928206086158752\n",
      "Batch 4620/4897, Loss: 0.6919764280319214\n",
      "Batch 4630/4897, Loss: 0.6841806173324585\n",
      "Batch 4640/4897, Loss: 0.6875345706939697\n",
      "Batch 4650/4897, Loss: 0.6899945735931396\n",
      "Batch 4660/4897, Loss: 0.6960161924362183\n",
      "Batch 4670/4897, Loss: 0.699678897857666\n",
      "Batch 4680/4897, Loss: 0.7002196311950684\n",
      "Batch 4690/4897, Loss: 0.6943216919898987\n",
      "Batch 4700/4897, Loss: 0.6885265111923218\n",
      "Batch 4710/4897, Loss: 0.6877114772796631\n",
      "Batch 4720/4897, Loss: 0.7067902684211731\n",
      "Batch 4730/4897, Loss: 0.6968187093734741\n",
      "Batch 4740/4897, Loss: 0.6937532424926758\n",
      "Batch 4750/4897, Loss: 0.689260721206665\n",
      "Batch 4760/4897, Loss: 0.6939011812210083\n",
      "Batch 4770/4897, Loss: 0.6981228590011597\n",
      "Batch 4780/4897, Loss: 0.6962411403656006\n",
      "Batch 4790/4897, Loss: 0.690392017364502\n",
      "Batch 4800/4897, Loss: 0.6938834190368652\n",
      "Batch 4810/4897, Loss: 0.6930254101753235\n",
      "Batch 4820/4897, Loss: 0.699087917804718\n",
      "Batch 4830/4897, Loss: 0.6964398622512817\n",
      "Batch 4840/4897, Loss: 0.6949918270111084\n",
      "Batch 4850/4897, Loss: 0.6823882460594177\n",
      "Batch 4860/4897, Loss: 0.6808097958564758\n",
      "Batch 4870/4897, Loss: 0.7079653143882751\n",
      "Batch 4880/4897, Loss: 0.6889661550521851\n",
      "Batch 4890/4897, Loss: 0.6815750002861023\n",
      "Epoch 2/2, Loss: 0.6861173510551453\n"
     ]
    }
   ],
   "source": [
    "# Train the neural network model\n",
    "num_epochs = 2  # Reduzindo o número de épocas para teste inicial\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Define o modelo para modo de treinamento\n",
    "    for batch_idx, (user_ids, item_ids, labels) in enumerate(train_loader):\n",
    "        user_ids = user_ids.to(device)  # Move os IDs de usuário para o dispositivo (CPU)\n",
    "        item_ids = item_ids.to(device)  # Move os IDs de item para o dispositivo (CPU)\n",
    "        labels = labels.to(device).float()  # Move as labels para o dispositivo (CPU) e as converte para float\n",
    "        optimizer.zero_grad()  # Zera os gradientes do otimizador\n",
    "        outputs = model(user_ids, item_ids).squeeze()  # Faz a previsão com o modelo\n",
    "        loss = criterion(outputs, labels)  # Calcula a perda\n",
    "        loss.backward()  # Propaga os gradientes\n",
    "        optimizer.step()  # Atualiza os pesos do modelo\n",
    "        if batch_idx % 10 == 0:  # Adiciona checkpoints de progresso\n",
    "            print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}')\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2aaff661-218e-40be-ad62-819ed350eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get recommendations\n",
    "def get_recommendations(user_id, num_recommendations):\n",
    "    user_idx = torch.tensor([user_id]).to(device)\n",
    "    item_indices = torch.arange(num_items).to(device)\n",
    "    \n",
    "    print(f\"User ID (encoded): {user_idx}\")\n",
    "    print(f\"Item Indices: {item_indices}\")\n",
    "    \n",
    "    user_idx_repeated = user_idx.repeat(item_indices.shape[0])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = model(user_idx_repeated, item_indices).squeeze()\n",
    "    \n",
    "    print(f\"Scores: {scores}\")\n",
    "    \n",
    "    _, top_item_indices = torch.topk(scores, num_recommendations)\n",
    "    \n",
    "    print(f\"Top Item Indices: {top_item_indices}\")\n",
    "    \n",
    "    recommended_item_ids = item_encoder.inverse_transform(top_item_indices.cpu().numpy())\n",
    "    \n",
    "    print(f\"Recommended Item IDs: {recommended_item_ids}\")\n",
    "    \n",
    "    return recommended_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "704a6225-475b-4a84-96f2-871fff702033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(test_df, num_recommendations):\n",
    "    hits = 0\n",
    "    total_relevant = 0\n",
    "    total_recommended = 0\n",
    "    \n",
    "    for user_id in test_df['user'].unique():\n",
    "        actual_items = set(test_df[test_df['user'] == user_id]['item'].unique())\n",
    "        recommended_items = set(get_recommendations(user_id, num_recommendations))\n",
    "        \n",
    "        hits += len(actual_items.intersection(recommended_items))\n",
    "        total_relevant += len(actual_items)\n",
    "        total_recommended += len(recommended_items)\n",
    "    \n",
    "    precision = hits / total_recommended if total_recommended > 0 else 0\n",
    "    recall = hits / total_relevant if total_relevant > 0 else 0\n",
    "    return precision, recall, hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "233314d5-464a-4336-87a0-6907b02350f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID (encoded): tensor([45476])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4951, 0.4762, 0.5277,  ..., 0.5015, 0.4563, 0.5388])\n",
      "Top Item Indices: tensor([ 6829, 25831, 37279, 35718,  6352])\n",
      "Recommended Item IDs: [ 75188 281826 407396 389607  70175]\n",
      "User ID (encoded): tensor([44790])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4990, 0.4874, 0.5351,  ..., 0.5038, 0.4674, 0.5281])\n",
      "Top Item Indices: tensor([37279,  3187, 24763, 26272, 41868])\n",
      "Recommended Item IDs: [407396  35202 270291 286647 458265]\n",
      "User ID (encoded): tensor([26548])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4843, 0.4823, 0.5252,  ..., 0.4950, 0.4706, 0.5244])\n",
      "Top Item Indices: tensor([37279, 33559,  3187, 25831, 35718])\n",
      "Recommended Item IDs: [407396 366879  35202 281826 389607]\n",
      "User ID (encoded): tensor([12436])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4761, 0.4914, 0.4955,  ..., 0.4959, 0.4952, 0.5013])\n",
      "Top Item Indices: tensor([15420, 40665,  3187, 17905,   486])\n",
      "Recommended Item IDs: [169956 445105  35202 197656   5411]\n",
      "User ID (encoded): tensor([13889])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5186, 0.4797, 0.5564,  ..., 0.5158, 0.4397, 0.5552])\n",
      "Top Item Indices: tensor([37279, 24071,  6829,  3187,  7770])\n",
      "Recommended Item IDs: [407396 262903  75188  35202  84734]\n",
      "User ID (encoded): tensor([29410])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4927, 0.4849, 0.5221,  ..., 0.4927, 0.4675, 0.5270])\n",
      "Top Item Indices: tensor([37279, 25831,  3187, 24071, 35718])\n",
      "Recommended Item IDs: [407396 281826  35202 262903 389607]\n",
      "User ID (encoded): tensor([64238])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5115, 0.4784, 0.5500,  ..., 0.5088, 0.4387, 0.5513])\n",
      "Top Item Indices: tensor([ 6829, 37279,  3187, 24071,  7770])\n",
      "Recommended Item IDs: [ 75188 407396  35202 262903  84734]\n",
      "User ID (encoded): tensor([4038])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5173, 0.4709, 0.5574,  ..., 0.5027, 0.4343, 0.5549])\n",
      "Top Item Indices: tensor([ 6829, 37279, 25831,  3187, 15420])\n",
      "Recommended Item IDs: [ 75188 407396 281826  35202 169956]\n",
      "User ID (encoded): tensor([54269])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5064, 0.4578, 0.5608,  ..., 0.4821, 0.4213, 0.5396])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 24763,  6352])\n",
      "Recommended Item IDs: [407396  75188  35202 270291  70175]\n",
      "User ID (encoded): tensor([58003])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4981, 0.4858, 0.5371,  ..., 0.4992, 0.4693, 0.5361])\n",
      "Top Item Indices: tensor([37279,  3187, 24071, 26272, 41868])\n",
      "Recommended Item IDs: [407396  35202 262903 286647 458265]\n",
      "User ID (encoded): tensor([24604])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5041, 0.4814, 0.5382,  ..., 0.5009, 0.4547, 0.5298])\n",
      "Top Item Indices: tensor([ 6829, 37279, 25831, 24071,  3187])\n",
      "Recommended Item IDs: [ 75188 407396 281826 262903  35202]\n",
      "User ID (encoded): tensor([2124])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4916, 0.4843, 0.5334,  ..., 0.5022, 0.4749, 0.5340])\n",
      "Top Item Indices: tensor([ 3187, 24763, 15420, 33309, 37279])\n",
      "Recommended Item IDs: [ 35202 270291 169956 364074 407396]\n",
      "User ID (encoded): tensor([34691])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4976, 0.4739, 0.5278,  ..., 0.4980, 0.4419, 0.5282])\n",
      "Top Item Indices: tensor([ 6829, 25831, 31182, 35718, 33237])\n",
      "Recommended Item IDs: [ 75188 281826 340808 389607 363193]\n",
      "User ID (encoded): tensor([3094])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4978, 0.4904, 0.5249,  ..., 0.4992, 0.4683, 0.5299])\n",
      "Top Item Indices: tensor([37279,  3187, 24071, 24763, 15420])\n",
      "Recommended Item IDs: [407396  35202 262903 270291 169956]\n",
      "User ID (encoded): tensor([62955])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5231, 0.4775, 0.5452,  ..., 0.4938, 0.4421, 0.5430])\n",
      "Top Item Indices: tensor([37279,  6829, 25831,  8323,  3187])\n",
      "Recommended Item IDs: [407396  75188 281826  90822  35202]\n",
      "User ID (encoded): tensor([50513])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5119, 0.4811, 0.5506,  ..., 0.5015, 0.4454, 0.5359])\n",
      "Top Item Indices: tensor([ 6829, 37279,  3187, 24071, 24763])\n",
      "Recommended Item IDs: [ 75188 407396  35202 262903 270291]\n",
      "User ID (encoded): tensor([56114])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5000, 0.4810, 0.5392,  ..., 0.5032, 0.4535, 0.5432])\n",
      "Top Item Indices: tensor([37279, 24071,  3187, 26272, 41868])\n",
      "Recommended Item IDs: [407396 262903  35202 286647 458265]\n",
      "User ID (encoded): tensor([14942])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5141, 0.4602, 0.5531,  ..., 0.4923, 0.4231, 0.5534])\n",
      "Top Item Indices: tensor([ 6829, 25831, 37279, 35718,  3187])\n",
      "Recommended Item IDs: [ 75188 281826 407396 389607  35202]\n",
      "User ID (encoded): tensor([8688])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5061, 0.4782, 0.5356,  ..., 0.4908, 0.4554, 0.5325])\n",
      "Top Item Indices: tensor([25831, 35718, 37279, 24071, 37614])\n",
      "Recommended Item IDs: [281826 389607 407396 262903 411198]\n",
      "User ID (encoded): tensor([51530])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5088, 0.4624, 0.5569,  ..., 0.4912, 0.4288, 0.5389])\n",
      "Top Item Indices: tensor([37279,  6829,  6352, 24763,  3187])\n",
      "Recommended Item IDs: [407396  75188  70175 270291  35202]\n",
      "User ID (encoded): tensor([15925])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4952, 0.4894, 0.5156,  ..., 0.4976, 0.4909, 0.5171])\n",
      "Top Item Indices: tensor([ 3187, 15420,  3497, 40665, 20182])\n",
      "Recommended Item IDs: [ 35202 169956  38706 445105 221742]\n",
      "User ID (encoded): tensor([59079])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4969, 0.4824, 0.5332,  ..., 0.4974, 0.4471, 0.5279])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 24763, 24071])\n",
      "Recommended Item IDs: [407396  75188  35202 270291 262903]\n",
      "User ID (encoded): tensor([37566])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5031, 0.4733, 0.5405,  ..., 0.5035, 0.4535, 0.5440])\n",
      "Top Item Indices: tensor([25831,  8323,  6829, 33559, 37279])\n",
      "Recommended Item IDs: [281826  90822  75188 366879 407396]\n",
      "User ID (encoded): tensor([32741])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4929, 0.4894, 0.5232,  ..., 0.4972, 0.4687, 0.5171])\n",
      "Top Item Indices: tensor([37279, 25831,  3187, 35718,  1935])\n",
      "Recommended Item IDs: [407396 281826  35202 389607  21730]\n",
      "User ID (encoded): tensor([9621])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5168, 0.4807, 0.5502,  ..., 0.5056, 0.4410, 0.5487])\n",
      "Top Item Indices: tensor([ 6829, 37279, 24071, 31182,  3187])\n",
      "Recommended Item IDs: [ 75188 407396 262903 340808  35202]\n",
      "User ID (encoded): tensor([57290])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5027, 0.4773, 0.5395,  ..., 0.5063, 0.4412, 0.5395])\n",
      "Top Item Indices: tensor([ 6829, 37279,  6352, 24763, 25831])\n",
      "Recommended Item IDs: [ 75188 407396  70175 270291 281826]\n",
      "User ID (encoded): tensor([47941])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4971, 0.4788, 0.5353,  ..., 0.4985, 0.4512, 0.5238])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 24763,  6352])\n",
      "Recommended Item IDs: [407396  75188  35202 270291  70175]\n",
      "User ID (encoded): tensor([56091])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4852, 0.4877, 0.5183,  ..., 0.5008, 0.4805, 0.5186])\n",
      "Top Item Indices: tensor([33559, 37279,  3187, 11214, 16607])\n",
      "Recommended Item IDs: [366879 407396  35202 122546 183066]\n",
      "User ID (encoded): tensor([50963])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4771, 0.4761, 0.5218,  ..., 0.4933, 0.4743, 0.5251])\n",
      "Top Item Indices: tensor([33559, 33237, 27581, 17053, 11214])\n",
      "Recommended Item IDs: [366879 363193 301295 188051 122546]\n",
      "User ID (encoded): tensor([48902])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5142, 0.4725, 0.5456,  ..., 0.5053, 0.4525, 0.5454])\n",
      "Top Item Indices: tensor([ 6829, 25831, 37279, 35718, 24071])\n",
      "Recommended Item IDs: [ 75188 281826 407396 389607 262903]\n",
      "User ID (encoded): tensor([42231])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5001, 0.4922, 0.5288,  ..., 0.4963, 0.4681, 0.5255])\n",
      "Top Item Indices: tensor([37279,  3187, 25831,  1935, 35718])\n",
      "Recommended Item IDs: [407396  35202 281826  21730 389607]\n",
      "User ID (encoded): tensor([54936])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4727, 0.4672, 0.5200,  ..., 0.4847, 0.4641, 0.5282])\n",
      "Top Item Indices: tensor([20675, 33559,  9660, 28166, 33237])\n",
      "Recommended Item IDs: [227152 366879 105347 307695 363193]\n",
      "User ID (encoded): tensor([7155])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5176, 0.4726, 0.5454,  ..., 0.4940, 0.4412, 0.5432])\n",
      "Top Item Indices: tensor([ 6829,  8323, 25831, 21739, 24071])\n",
      "Recommended Item IDs: [ 75188  90822 281826 237992 262903]\n",
      "User ID (encoded): tensor([14091])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5017, 0.4799, 0.5414,  ..., 0.5045, 0.4501, 0.5426])\n",
      "Top Item Indices: tensor([37279,  3187, 24071, 24763,  6352])\n",
      "Recommended Item IDs: [407396  35202 262903 270291  70175]\n",
      "User ID (encoded): tensor([23557])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4798, 0.4762, 0.5218,  ..., 0.4945, 0.4714, 0.5225])\n",
      "Top Item Indices: tensor([33559, 37279,  3187, 25831, 28166])\n",
      "Recommended Item IDs: [366879 407396  35202 281826 307695]\n",
      "User ID (encoded): tensor([15650])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5168, 0.4777, 0.5450,  ..., 0.5020, 0.4570, 0.5436])\n",
      "Top Item Indices: tensor([37279,  3187, 25831, 24071,  6829])\n",
      "Recommended Item IDs: [407396  35202 281826 262903  75188]\n",
      "User ID (encoded): tensor([44620])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5031, 0.4683, 0.5439,  ..., 0.5104, 0.4482, 0.5400])\n",
      "Top Item Indices: tensor([ 6829, 37279,  6352, 24071, 22601])\n",
      "Recommended Item IDs: [ 75188 407396  70175 262903 247224]\n",
      "User ID (encoded): tensor([37705])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5095, 0.4679, 0.5437,  ..., 0.4970, 0.4290, 0.5501])\n",
      "Top Item Indices: tensor([ 6829, 37279, 25831,  6352,  7155])\n",
      "Recommended Item IDs: [ 75188 407396 281826  70175  78476]\n",
      "User ID (encoded): tensor([15284])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4964, 0.4772, 0.5392,  ..., 0.5049, 0.4466, 0.5414])\n",
      "Top Item Indices: tensor([37279,  6352, 24071, 24763,  7155])\n",
      "Recommended Item IDs: [407396  70175 262903 270291  78476]\n",
      "User ID (encoded): tensor([12298])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5161, 0.4766, 0.5542,  ..., 0.5000, 0.4370, 0.5399])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 24763,  6352])\n",
      "Recommended Item IDs: [407396  75188  35202 270291  70175]\n",
      "User ID (encoded): tensor([59124])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4817, 0.4807, 0.5251,  ..., 0.5034, 0.4867, 0.5166])\n",
      "Top Item Indices: tensor([ 3187, 15420, 24763,  9357, 40665])\n",
      "Recommended Item IDs: [ 35202 169956 270291 102140 445105]\n",
      "User ID (encoded): tensor([8939])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5072, 0.4762, 0.5493,  ..., 0.5023, 0.4514, 0.5328])\n",
      "Top Item Indices: tensor([37279,  3187, 24071, 24763, 26272])\n",
      "Recommended Item IDs: [407396  35202 262903 270291 286647]\n",
      "User ID (encoded): tensor([47441])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5144, 0.4810, 0.5493,  ..., 0.5074, 0.4439, 0.5488])\n",
      "Top Item Indices: tensor([37279,  3187, 24763, 24071, 15420])\n",
      "Recommended Item IDs: [407396  35202 270291 262903 169956]\n",
      "User ID (encoded): tensor([56270])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5156, 0.4678, 0.5478,  ..., 0.4944, 0.4331, 0.5466])\n",
      "Top Item Indices: tensor([ 6829, 37279, 21739, 25831,  3187])\n",
      "Recommended Item IDs: [ 75188 407396 237992 281826  35202]\n",
      "User ID (encoded): tensor([7671])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4891, 0.4819, 0.5187,  ..., 0.4936, 0.4639, 0.5273])\n",
      "Top Item Indices: tensor([25831, 37279, 23773, 35718,  6352])\n",
      "Recommended Item IDs: [281826 407396 259847 389607  70175]\n",
      "User ID (encoded): tensor([52213])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5153, 0.4760, 0.5555,  ..., 0.5095, 0.4286, 0.5611])\n",
      "Top Item Indices: tensor([ 6829, 37279,  3187,  7770,  6352])\n",
      "Recommended Item IDs: [ 75188 407396  35202  84734  70175]\n",
      "User ID (encoded): tensor([37634])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4996, 0.4706, 0.5531,  ..., 0.5114, 0.4506, 0.5515])\n",
      "Top Item Indices: tensor([ 7155, 37279, 22601, 27555,  6352])\n",
      "Recommended Item IDs: [ 78476 407396 247224 301019  70175]\n",
      "User ID (encoded): tensor([24320])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5013, 0.4692, 0.5345,  ..., 0.4941, 0.4388, 0.5333])\n",
      "Top Item Indices: tensor([ 6829, 25831, 37279,  3187, 35718])\n",
      "Recommended Item IDs: [ 75188 281826 407396  35202 389607]\n",
      "User ID (encoded): tensor([44613])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5076, 0.4628, 0.5418,  ..., 0.4987, 0.4245, 0.5475])\n",
      "Top Item Indices: tensor([ 6829, 25831, 35718, 37279,  6352])\n",
      "Recommended Item IDs: [ 75188 281826 389607 407396  70175]\n",
      "User ID (encoded): tensor([42736])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4907, 0.4869, 0.5229,  ..., 0.5025, 0.4786, 0.5257])\n",
      "Top Item Indices: tensor([ 3187, 37279, 18628, 24763, 15420])\n",
      "Recommended Item IDs: [ 35202 407396 205383 270291 169956]\n",
      "User ID (encoded): tensor([30992])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5103, 0.4790, 0.5465,  ..., 0.5096, 0.4427, 0.5502])\n",
      "Top Item Indices: tensor([37279,  6829, 24071,  3187, 24763])\n",
      "Recommended Item IDs: [407396  75188 262903  35202 270291]\n",
      "User ID (encoded): tensor([66070])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5086, 0.4826, 0.5396,  ..., 0.4973, 0.4539, 0.5355])\n",
      "Top Item Indices: tensor([37279,  6829, 24071,  3187, 25831])\n",
      "Recommended Item IDs: [407396  75188 262903  35202 281826]\n",
      "User ID (encoded): tensor([33604])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5142, 0.4594, 0.5627,  ..., 0.4951, 0.4326, 0.5419])\n",
      "Top Item Indices: tensor([ 6829, 37279,  3187,  6352, 24763])\n",
      "Recommended Item IDs: [ 75188 407396  35202  70175 270291]\n",
      "User ID (encoded): tensor([40536])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4973, 0.4786, 0.5386,  ..., 0.5038, 0.4674, 0.5320])\n",
      "Top Item Indices: tensor([37279,  6829, 24071, 25831,  3187])\n",
      "Recommended Item IDs: [407396  75188 262903 281826  35202]\n",
      "User ID (encoded): tensor([2743])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5141, 0.4802, 0.5518,  ..., 0.5106, 0.4412, 0.5516])\n",
      "Top Item Indices: tensor([37279,  3187, 24071,  6829, 24763])\n",
      "Recommended Item IDs: [407396  35202 262903  75188 270291]\n",
      "User ID (encoded): tensor([62047])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4846, 0.4834, 0.5226,  ..., 0.4891, 0.4691, 0.5281])\n",
      "Top Item Indices: tensor([37279,  7155,  3187,  6352, 24071])\n",
      "Recommended Item IDs: [407396  78476  35202  70175 262903]\n",
      "User ID (encoded): tensor([31987])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5093, 0.4669, 0.5453,  ..., 0.4963, 0.4269, 0.5513])\n",
      "Top Item Indices: tensor([ 6829, 37279, 25831,  6352,  7155])\n",
      "Recommended Item IDs: [ 75188 407396 281826  70175  78476]\n",
      "User ID (encoded): tensor([47905])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5011, 0.4856, 0.5393,  ..., 0.4976, 0.4547, 0.5334])\n",
      "Top Item Indices: tensor([37279,  3187, 24763,  6352, 24071])\n",
      "Recommended Item IDs: [407396  35202 270291  70175 262903]\n",
      "User ID (encoded): tensor([20471])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5071, 0.4763, 0.5422,  ..., 0.4976, 0.4440, 0.5341])\n",
      "Top Item Indices: tensor([ 6829, 37279,  3187, 24071, 25831])\n",
      "Recommended Item IDs: [ 75188 407396  35202 262903 281826]\n",
      "User ID (encoded): tensor([54290])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5158, 0.4729, 0.5448,  ..., 0.4965, 0.4408, 0.5413])\n",
      "Top Item Indices: tensor([ 6829, 25831,  8323, 21739, 31182])\n",
      "Recommended Item IDs: [ 75188 281826  90822 237992 340808]\n",
      "User ID (encoded): tensor([28223])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4975, 0.4806, 0.5306,  ..., 0.5000, 0.4511, 0.5393])\n",
      "Top Item Indices: tensor([ 6829, 37279, 24071, 24763, 25831])\n",
      "Recommended Item IDs: [ 75188 407396 262903 270291 281826]\n",
      "User ID (encoded): tensor([16205])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5180, 0.4712, 0.5468,  ..., 0.5033, 0.4290, 0.5411])\n",
      "Top Item Indices: tensor([ 3187, 15420, 24763, 40665, 40715])\n",
      "Recommended Item IDs: [ 35202 169956 270291 445105 445681]\n",
      "User ID (encoded): tensor([4573])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4847, 0.4845, 0.5289,  ..., 0.4975, 0.4833, 0.5333])\n",
      "Top Item Indices: tensor([17053, 24056, 33559, 13136, 37497])\n",
      "Recommended Item IDs: [188051 262752 366879 144323 409804]\n",
      "User ID (encoded): tensor([41208])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5124, 0.4698, 0.5452,  ..., 0.5129, 0.4505, 0.5425])\n",
      "Top Item Indices: tensor([ 6829, 37279, 25831, 24071,  3187])\n",
      "Recommended Item IDs: [ 75188 407396 281826 262903  35202]\n",
      "User ID (encoded): tensor([62061])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4946, 0.4749, 0.5287,  ..., 0.5024, 0.4565, 0.5299])\n",
      "Top Item Indices: tensor([33559,  6829,  3187, 25831, 37279])\n",
      "Recommended Item IDs: [366879  75188  35202 281826 407396]\n",
      "User ID (encoded): tensor([19763])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4751, 0.4707, 0.5245,  ..., 0.4900, 0.4627, 0.5375])\n",
      "Top Item Indices: tensor([37279, 33559,  7155,  6352, 37145])\n",
      "Recommended Item IDs: [407396 366879  78476  70175 405862]\n",
      "User ID (encoded): tensor([51366])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4922, 0.4770, 0.5384,  ..., 0.5012, 0.4540, 0.5482])\n",
      "Top Item Indices: tensor([37279, 24071,  7155,  6352, 41868])\n",
      "Recommended Item IDs: [407396 262903  78476  70175 458265]\n",
      "User ID (encoded): tensor([51893])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4937, 0.4867, 0.5285,  ..., 0.4947, 0.4723, 0.5331])\n",
      "Top Item Indices: tensor([37279, 25831, 33559, 35718, 24071])\n",
      "Recommended Item IDs: [407396 281826 366879 389607 262903]\n",
      "User ID (encoded): tensor([36232])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4890, 0.4851, 0.5120,  ..., 0.4917, 0.4670, 0.5217])\n",
      "Top Item Indices: tensor([25831, 37279, 35718, 23773, 31182])\n",
      "Recommended Item IDs: [281826 407396 389607 259847 340808]\n",
      "User ID (encoded): tensor([61975])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5109, 0.4698, 0.5510,  ..., 0.5029, 0.4454, 0.5325])\n",
      "Top Item Indices: tensor([ 6829, 37279,  3187,  6352, 24071])\n",
      "Recommended Item IDs: [ 75188 407396  35202  70175 262903]\n",
      "User ID (encoded): tensor([14900])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4923, 0.4882, 0.5314,  ..., 0.5026, 0.4666, 0.5314])\n",
      "Top Item Indices: tensor([37279, 41868,  3187, 26272, 33309])\n",
      "Recommended Item IDs: [407396 458265  35202 286647 364074]\n",
      "User ID (encoded): tensor([13415])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5009, 0.4760, 0.5474,  ..., 0.5078, 0.4529, 0.5362])\n",
      "Top Item Indices: tensor([37279,  7155, 41868, 33309, 26272])\n",
      "Recommended Item IDs: [407396  78476 458265 364074 286647]\n",
      "User ID (encoded): tensor([19667])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4830, 0.4813, 0.5293,  ..., 0.4986, 0.4722, 0.5290])\n",
      "Top Item Indices: tensor([37279,  3187, 33309, 26272, 41868])\n",
      "Recommended Item IDs: [407396  35202 364074 286647 458265]\n",
      "User ID (encoded): tensor([28297])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5003, 0.4742, 0.5346,  ..., 0.4934, 0.4425, 0.5312])\n",
      "Top Item Indices: tensor([37279,  6829, 25831,  7711, 35718])\n",
      "Recommended Item IDs: [407396  75188 281826  84105 389607]\n",
      "User ID (encoded): tensor([25967])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4962, 0.4830, 0.5327,  ..., 0.4994, 0.4600, 0.5208])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 25831, 24763])\n",
      "Recommended Item IDs: [407396  75188  35202 281826 270291]\n",
      "User ID (encoded): tensor([67769])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5177, 0.4721, 0.5623,  ..., 0.5052, 0.4241, 0.5601])\n",
      "Top Item Indices: tensor([ 6829, 37279,  3187, 24763,  6352])\n",
      "Recommended Item IDs: [ 75188 407396  35202 270291  70175]\n",
      "User ID (encoded): tensor([7812])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5087, 0.4967, 0.5362,  ..., 0.4979, 0.4662, 0.5372])\n",
      "Top Item Indices: tensor([37279,  3187, 26272, 41868, 33309])\n",
      "Recommended Item IDs: [407396  35202 286647 458265 364074]\n",
      "User ID (encoded): tensor([52003])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5003, 0.4896, 0.5343,  ..., 0.5051, 0.4763, 0.5289])\n",
      "Top Item Indices: tensor([ 3187, 24763, 33309, 40715, 15420])\n",
      "Recommended Item IDs: [ 35202 270291 364074 445681 169956]\n",
      "User ID (encoded): tensor([59364])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4980, 0.4818, 0.5292,  ..., 0.4864, 0.4607, 0.5236])\n",
      "Top Item Indices: tensor([25831, 35718, 37279, 31182, 24071])\n",
      "Recommended Item IDs: [281826 389607 407396 340808 262903]\n",
      "User ID (encoded): tensor([49905])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4858, 0.4835, 0.5164,  ..., 0.4849, 0.4713, 0.5229])\n",
      "Top Item Indices: tensor([37279,  3187,  6352, 24071, 24763])\n",
      "Recommended Item IDs: [407396  35202  70175 262903 270291]\n",
      "User ID (encoded): tensor([67087])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4834, 0.4858, 0.5153,  ..., 0.4901, 0.4759, 0.5202])\n",
      "Top Item Indices: tensor([33559, 33237, 11214, 37279,  6496])\n",
      "Recommended Item IDs: [366879 363193 122546 407396  71698]\n",
      "User ID (encoded): tensor([30131])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4808, 0.4810, 0.5208,  ..., 0.4971, 0.4654, 0.5289])\n",
      "Top Item Indices: tensor([33559, 33237, 17053, 11214, 27581])\n",
      "Recommended Item IDs: [366879 363193 188051 122546 301295]\n",
      "User ID (encoded): tensor([21084])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4952, 0.4789, 0.5254,  ..., 0.4989, 0.4531, 0.5366])\n",
      "Top Item Indices: tensor([ 6829, 37279, 25831, 24071, 35718])\n",
      "Recommended Item IDs: [ 75188 407396 281826 262903 389607]\n",
      "User ID (encoded): tensor([57172])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5012, 0.4788, 0.5334,  ..., 0.4851, 0.4518, 0.5270])\n",
      "Top Item Indices: tensor([25831, 21739, 35718, 11040,  1935])\n",
      "Recommended Item IDs: [281826 237992 389607 120471  21730]\n",
      "User ID (encoded): tensor([25486])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4992, 0.4771, 0.5383,  ..., 0.5063, 0.4567, 0.5377])\n",
      "Top Item Indices: tensor([ 6829, 37279, 25831, 30265,  3187])\n",
      "Recommended Item IDs: [ 75188 407396 281826 331459  35202]\n",
      "User ID (encoded): tensor([41875])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4928, 0.4714, 0.5283,  ..., 0.5020, 0.4562, 0.5398])\n",
      "Top Item Indices: tensor([23773,  6829, 37279, 24071, 25831])\n",
      "Recommended Item IDs: [259847  75188 407396 262903 281826]\n",
      "User ID (encoded): tensor([50593])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5051, 0.4713, 0.5361,  ..., 0.4998, 0.4555, 0.5394])\n",
      "Top Item Indices: tensor([ 6829, 25831, 37279,  8323,  3187])\n",
      "Recommended Item IDs: [ 75188 281826 407396  90822  35202]\n",
      "User ID (encoded): tensor([68596])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5090, 0.4794, 0.5453,  ..., 0.5079, 0.4480, 0.5473])\n",
      "Top Item Indices: tensor([37279, 25831, 24071,  6829,  3187])\n",
      "Recommended Item IDs: [407396 281826 262903  75188  35202]\n",
      "User ID (encoded): tensor([39896])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5069, 0.4739, 0.5387,  ..., 0.4941, 0.4479, 0.5345])\n",
      "Top Item Indices: tensor([ 6829,  8323, 25831, 21739,  3187])\n",
      "Recommended Item IDs: [ 75188  90822 281826 237992  35202]\n",
      "User ID (encoded): tensor([27706])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4946, 0.4715, 0.5327,  ..., 0.5023, 0.4474, 0.5488])\n",
      "Top Item Indices: tensor([ 6829, 37279, 24071, 15420, 24763])\n",
      "Recommended Item IDs: [ 75188 407396 262903 169956 270291]\n",
      "User ID (encoded): tensor([15944])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4994, 0.4881, 0.5353,  ..., 0.5015, 0.4645, 0.5335])\n",
      "Top Item Indices: tensor([37279,  3187, 24071, 26272, 41868])\n",
      "Recommended Item IDs: [407396  35202 262903 286647 458265]\n",
      "User ID (encoded): tensor([27528])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5134, 0.4858, 0.5395,  ..., 0.4983, 0.4592, 0.5411])\n",
      "Top Item Indices: tensor([37279, 24071, 37614, 25831,  3187])\n",
      "Recommended Item IDs: [407396 262903 411198 281826  35202]\n",
      "User ID (encoded): tensor([57292])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5101, 0.4698, 0.5528,  ..., 0.4901, 0.4265, 0.5480])\n",
      "Top Item Indices: tensor([ 6829, 37279,  3187, 15420,  3675])\n",
      "Recommended Item IDs: [ 75188 407396  35202 169956  40864]\n",
      "User ID (encoded): tensor([54855])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5045, 0.4634, 0.5541,  ..., 0.4865, 0.4305, 0.5319])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 24763, 25831])\n",
      "Recommended Item IDs: [407396  75188  35202 270291 281826]\n",
      "User ID (encoded): tensor([48807])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5106, 0.4644, 0.5551,  ..., 0.4829, 0.4190, 0.5452])\n",
      "Top Item Indices: tensor([ 6829, 37279,  3187,  6352, 15420])\n",
      "Recommended Item IDs: [ 75188 407396  35202  70175 169956]\n",
      "User ID (encoded): tensor([62699])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4985, 0.4813, 0.5363,  ..., 0.4985, 0.4535, 0.5235])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 24763, 25831])\n",
      "Recommended Item IDs: [407396  75188  35202 270291 281826]\n",
      "User ID (encoded): tensor([9764])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5309, 0.4881, 0.5577,  ..., 0.5130, 0.4366, 0.5598])\n",
      "Top Item Indices: tensor([ 6829, 37279, 24071,  3187, 15420])\n",
      "Recommended Item IDs: [ 75188 407396 262903  35202 169956]\n",
      "User ID (encoded): tensor([10970])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5136, 0.4801, 0.5533,  ..., 0.4987, 0.4400, 0.5411])\n",
      "Top Item Indices: tensor([ 6829, 37279,  3187, 24071, 24763])\n",
      "Recommended Item IDs: [ 75188 407396  35202 262903 270291]\n",
      "User ID (encoded): tensor([903])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4988, 0.4761, 0.5334,  ..., 0.4984, 0.4476, 0.5283])\n",
      "Top Item Indices: tensor([31182, 25831,  6829, 37279, 35718])\n",
      "Recommended Item IDs: [340808 281826  75188 407396 389607]\n",
      "User ID (encoded): tensor([10864])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4957, 0.4894, 0.5286,  ..., 0.4962, 0.4611, 0.5263])\n",
      "Top Item Indices: tensor([ 3187, 37279, 25831,  1935, 35718])\n",
      "Recommended Item IDs: [ 35202 407396 281826  21730 389607]\n",
      "User ID (encoded): tensor([11412])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5081, 0.4746, 0.5431,  ..., 0.5018, 0.4470, 0.5380])\n",
      "Top Item Indices: tensor([25831, 31182, 37279, 35718, 24071])\n",
      "Recommended Item IDs: [281826 340808 407396 389607 262903]\n",
      "User ID (encoded): tensor([34224])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5015, 0.4767, 0.5357,  ..., 0.5048, 0.4405, 0.5469])\n",
      "Top Item Indices: tensor([ 6829, 37279, 24071, 24763,  6352])\n",
      "Recommended Item IDs: [ 75188 407396 262903 270291  70175]\n",
      "User ID (encoded): tensor([55557])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5220, 0.4735, 0.5542,  ..., 0.5008, 0.4435, 0.5381])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 24071, 24763])\n",
      "Recommended Item IDs: [407396  75188  35202 262903 270291]\n",
      "User ID (encoded): tensor([20090])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5036, 0.4794, 0.5389,  ..., 0.5062, 0.4506, 0.5415])\n",
      "Top Item Indices: tensor([ 6829, 37279, 24071, 31182, 25831])\n",
      "Recommended Item IDs: [ 75188 407396 262903 340808 281826]\n",
      "User ID (encoded): tensor([68167])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4982, 0.4750, 0.5399,  ..., 0.5064, 0.4590, 0.5364])\n",
      "Top Item Indices: tensor([ 6829, 37279, 24071, 25831,  3187])\n",
      "Recommended Item IDs: [ 75188 407396 262903 281826  35202]\n",
      "User ID (encoded): tensor([18086])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5079, 0.4634, 0.5549,  ..., 0.4855, 0.4221, 0.5402])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 24763,  6352])\n",
      "Recommended Item IDs: [407396  75188  35202 270291  70175]\n",
      "User ID (encoded): tensor([56197])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4944, 0.4880, 0.5301,  ..., 0.5020, 0.4643, 0.5284])\n",
      "Top Item Indices: tensor([37279,  3187, 24071, 24056, 26272])\n",
      "Recommended Item IDs: [407396  35202 262903 262752 286647]\n",
      "User ID (encoded): tensor([56417])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4978, 0.4786, 0.5404,  ..., 0.5048, 0.4532, 0.5430])\n",
      "Top Item Indices: tensor([37279, 24071,  3187,  7155,  6352])\n",
      "Recommended Item IDs: [407396 262903  35202  78476  70175]\n",
      "User ID (encoded): tensor([30227])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4917, 0.4850, 0.5284,  ..., 0.4972, 0.4693, 0.5306])\n",
      "Top Item Indices: tensor([37279, 25831, 35718,  3187, 24071])\n",
      "Recommended Item IDs: [407396 281826 389607  35202 262903]\n",
      "User ID (encoded): tensor([21765])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4929, 0.4837, 0.5273,  ..., 0.4974, 0.4616, 0.5260])\n",
      "Top Item Indices: tensor([37279,  3187, 24071,  6352, 25831])\n",
      "Recommended Item IDs: [407396  35202 262903  70175 281826]\n",
      "User ID (encoded): tensor([47038])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4873, 0.4851, 0.5216,  ..., 0.4913, 0.4698, 0.5252])\n",
      "Top Item Indices: tensor([37279,  3187, 25831,  6352, 24071])\n",
      "Recommended Item IDs: [407396  35202 281826  70175 262903]\n",
      "User ID (encoded): tensor([42962])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4999, 0.4862, 0.5309,  ..., 0.4969, 0.4608, 0.5267])\n",
      "Top Item Indices: tensor([ 3187, 25831, 37279,  1935, 35718])\n",
      "Recommended Item IDs: [ 35202 281826 407396  21730 389607]\n",
      "User ID (encoded): tensor([66297])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4959, 0.4394, 0.5619,  ..., 0.4781, 0.4050, 0.5385])\n",
      "Top Item Indices: tensor([37279, 24763,  6352,  6829,  7155])\n",
      "Recommended Item IDs: [407396 270291  70175  75188  78476]\n",
      "User ID (encoded): tensor([47438])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5055, 0.4732, 0.5482,  ..., 0.5031, 0.4431, 0.5369])\n",
      "Top Item Indices: tensor([37279,  7155, 41868, 26272,  3187])\n",
      "Recommended Item IDs: [407396  78476 458265 286647  35202]\n",
      "User ID (encoded): tensor([9836])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4968, 0.4799, 0.5289,  ..., 0.5021, 0.4483, 0.5359])\n",
      "Top Item Indices: tensor([ 6829, 37279, 25831, 24071, 31182])\n",
      "Recommended Item IDs: [ 75188 407396 281826 262903 340808]\n",
      "User ID (encoded): tensor([4116])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4858, 0.4831, 0.5310,  ..., 0.4962, 0.4838, 0.5300])\n",
      "Top Item Indices: tensor([17053,  6496, 13136, 24056,  6352])\n",
      "Recommended Item IDs: [188051  71698 144323 262752  70175]\n",
      "User ID (encoded): tensor([66549])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5071, 0.4789, 0.5422,  ..., 0.5066, 0.4491, 0.5440])\n",
      "Top Item Indices: tensor([ 6829, 37279, 24071, 31182, 25831])\n",
      "Recommended Item IDs: [ 75188 407396 262903 340808 281826]\n",
      "User ID (encoded): tensor([8375])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4953, 0.5014, 0.5169,  ..., 0.4990, 0.4681, 0.5351])\n",
      "Top Item Indices: tensor([ 3187, 18628, 24763, 15420, 21167])\n",
      "Recommended Item IDs: [ 35202 205383 270291 169956 232131]\n",
      "User ID (encoded): tensor([25882])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5035, 0.4817, 0.5438,  ..., 0.5016, 0.4544, 0.5273])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 24071, 24763])\n",
      "Recommended Item IDs: [407396  75188  35202 262903 270291]\n",
      "User ID (encoded): tensor([19432])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5047, 0.4812, 0.5406,  ..., 0.5046, 0.4548, 0.5393])\n",
      "Top Item Indices: tensor([37279,  6829, 24071, 25831, 31182])\n",
      "Recommended Item IDs: [407396  75188 262903 281826 340808]\n",
      "User ID (encoded): tensor([44428])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4805, 0.4875, 0.5088,  ..., 0.5047, 0.4969, 0.5061])\n",
      "Top Item Indices: tensor([13136,  3187, 36294, 25563, 31536])\n",
      "Recommended Item IDs: [144323  35202 395656 278997 344902]\n",
      "User ID (encoded): tensor([28292])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5022, 0.4762, 0.5353,  ..., 0.5057, 0.4424, 0.5391])\n",
      "Top Item Indices: tensor([ 6829, 37279, 25831, 24071, 35718])\n",
      "Recommended Item IDs: [ 75188 407396 281826 262903 389607]\n",
      "User ID (encoded): tensor([16274])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5063, 0.4980, 0.5343,  ..., 0.5050, 0.4682, 0.5400])\n",
      "Top Item Indices: tensor([ 3187, 15420, 24763, 40665,  3497])\n",
      "Recommended Item IDs: [ 35202 169956 270291 445105  38706]\n",
      "User ID (encoded): tensor([25388])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5092, 0.4681, 0.5511,  ..., 0.4998, 0.4314, 0.5399])\n",
      "Top Item Indices: tensor([37279,  7155,  3187,  6352, 24763])\n",
      "Recommended Item IDs: [407396  78476  35202  70175 270291]\n",
      "User ID (encoded): tensor([5824])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4936, 0.4832, 0.5262,  ..., 0.4975, 0.4615, 0.5280])\n",
      "Top Item Indices: tensor([ 6829, 37279, 25831,  3187, 35718])\n",
      "Recommended Item IDs: [ 75188 407396 281826  35202 389607]\n",
      "User ID (encoded): tensor([7027])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5008, 0.4757, 0.5456,  ..., 0.5077, 0.4464, 0.5539])\n",
      "Top Item Indices: tensor([37279, 24071, 41868, 26272,  6352])\n",
      "Recommended Item IDs: [407396 262903 458265 286647  70175]\n",
      "User ID (encoded): tensor([66811])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5005, 0.4751, 0.5448,  ..., 0.5063, 0.4496, 0.5548])\n",
      "Top Item Indices: tensor([37279,  7155, 24071, 26272, 41868])\n",
      "Recommended Item IDs: [407396  78476 262903 286647 458265]\n",
      "User ID (encoded): tensor([5299])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4771, 0.4735, 0.5173,  ..., 0.4891, 0.4676, 0.5233])\n",
      "Top Item Indices: tensor([23773, 33559,  6352, 37279, 24056])\n",
      "Recommended Item IDs: [259847 366879  70175 407396 262752]\n",
      "User ID (encoded): tensor([27304])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5001, 0.4748, 0.5335,  ..., 0.5044, 0.4492, 0.5338])\n",
      "Top Item Indices: tensor([ 6829, 25831, 37279,  3187, 35718])\n",
      "Recommended Item IDs: [ 75188 281826 407396  35202 389607]\n",
      "User ID (encoded): tensor([25374])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4872, 0.4926, 0.5181,  ..., 0.5014, 0.4641, 0.5230])\n",
      "Top Item Indices: tensor([15420, 40665, 40715,  3187, 18628])\n",
      "Recommended Item IDs: [169956 445105 445681  35202 205383]\n",
      "User ID (encoded): tensor([42726])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4932, 0.4838, 0.5277,  ..., 0.5019, 0.4551, 0.5205])\n",
      "Top Item Indices: tensor([37279,  6352, 24763,  7155,  3187])\n",
      "Recommended Item IDs: [407396  70175 270291  78476  35202]\n",
      "User ID (encoded): tensor([27110])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5097, 0.4664, 0.5530,  ..., 0.4902, 0.4330, 0.5358])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 24763,  6352])\n",
      "Recommended Item IDs: [407396  75188  35202 270291  70175]\n",
      "User ID (encoded): tensor([39806])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5056, 0.4844, 0.5369,  ..., 0.5005, 0.4613, 0.5346])\n",
      "Top Item Indices: tensor([37279, 24071,  3187, 25831, 37614])\n",
      "Recommended Item IDs: [407396 262903  35202 281826 411198]\n",
      "User ID (encoded): tensor([30922])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4849, 0.4763, 0.5290,  ..., 0.4969, 0.4645, 0.5342])\n",
      "Top Item Indices: tensor([37279,  7155, 26272, 22601, 41868])\n",
      "Recommended Item IDs: [407396  78476 286647 247224 458265]\n",
      "User ID (encoded): tensor([64763])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4998, 0.4790, 0.5353,  ..., 0.5002, 0.4547, 0.5289])\n",
      "Top Item Indices: tensor([25831, 37279, 35718,  3187, 24071])\n",
      "Recommended Item IDs: [281826 407396 389607  35202 262903]\n",
      "User ID (encoded): tensor([46757])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4900, 0.4957, 0.5051,  ..., 0.5019, 0.4817, 0.4998])\n",
      "Top Item Indices: tensor([15420, 40665,  9469, 40715,  3187])\n",
      "Recommended Item IDs: [169956 445105 103342 445681  35202]\n",
      "User ID (encoded): tensor([15172])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5263, 0.4821, 0.5467,  ..., 0.4946, 0.4464, 0.5449])\n",
      "Top Item Indices: tensor([25831,  3187, 35718, 37279, 11040])\n",
      "Recommended Item IDs: [281826  35202 389607 407396 120471]\n",
      "User ID (encoded): tensor([55883])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4984, 0.4850, 0.5360,  ..., 0.5012, 0.4630, 0.5320])\n",
      "Top Item Indices: tensor([37279, 25831,  3187, 35718, 24071])\n",
      "Recommended Item IDs: [407396 281826  35202 389607 262903]\n",
      "User ID (encoded): tensor([16200])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4920, 0.4892, 0.5240,  ..., 0.5079, 0.4815, 0.5271])\n",
      "Top Item Indices: tensor([15420,  3187, 40665, 40715, 24763])\n",
      "Recommended Item IDs: [169956  35202 445105 445681 270291]\n",
      "User ID (encoded): tensor([54688])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4924, 0.4768, 0.5302,  ..., 0.5012, 0.4519, 0.5366])\n",
      "Top Item Indices: tensor([ 6829, 37279, 24071, 24763,  6352])\n",
      "Recommended Item IDs: [ 75188 407396 262903 270291  70175]\n",
      "User ID (encoded): tensor([54959])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5108, 0.4800, 0.5488,  ..., 0.5083, 0.4528, 0.5402])\n",
      "Top Item Indices: tensor([37279, 24071,  6829,  3187, 26272])\n",
      "Recommended Item IDs: [407396 262903  75188  35202 286647]\n",
      "User ID (encoded): tensor([393])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5084, 0.4753, 0.5417,  ..., 0.5007, 0.4490, 0.5365])\n",
      "Top Item Indices: tensor([25831, 37279, 31182,  6829, 24071])\n",
      "Recommended Item IDs: [281826 407396 340808  75188 262903]\n",
      "User ID (encoded): tensor([44039])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5094, 0.4835, 0.5436,  ..., 0.5062, 0.4569, 0.5393])\n",
      "Top Item Indices: tensor([37279, 24071,  3187, 25831, 26272])\n",
      "Recommended Item IDs: [407396 262903  35202 281826 286647]\n",
      "User ID (encoded): tensor([6586])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5063, 0.4690, 0.5407,  ..., 0.5021, 0.4441, 0.5378])\n",
      "Top Item Indices: tensor([ 6829, 25831, 24071, 37279, 31182])\n",
      "Recommended Item IDs: [ 75188 281826 262903 407396 340808]\n",
      "User ID (encoded): tensor([41950])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4898, 0.4888, 0.5131,  ..., 0.5043, 0.4847, 0.5211])\n",
      "Top Item Indices: tensor([ 3187, 18628, 15420, 40665, 40715])\n",
      "Recommended Item IDs: [ 35202 205383 169956 445105 445681]\n",
      "User ID (encoded): tensor([66043])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4858, 0.4695, 0.5265,  ..., 0.4909, 0.4552, 0.5466])\n",
      "Top Item Indices: tensor([ 6829, 23773, 37279, 24056, 24071])\n",
      "Recommended Item IDs: [ 75188 259847 407396 262752 262903]\n",
      "User ID (encoded): tensor([58345])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4983, 0.4783, 0.5318,  ..., 0.4952, 0.4515, 0.5259])\n",
      "Top Item Indices: tensor([25831, 31182,  3187,  1935, 35718])\n",
      "Recommended Item IDs: [281826 340808  35202  21730 389607]\n",
      "User ID (encoded): tensor([924])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5123, 0.4772, 0.5441,  ..., 0.4985, 0.4458, 0.5400])\n",
      "Top Item Indices: tensor([ 6829, 37279, 24071,  8323,  3187])\n",
      "Recommended Item IDs: [ 75188 407396 262903  90822  35202]\n",
      "User ID (encoded): tensor([60719])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4801, 0.4919, 0.5066,  ..., 0.4969, 0.4828, 0.5207])\n",
      "Top Item Indices: tensor([ 3187, 18628,   486, 24763, 15420])\n",
      "Recommended Item IDs: [ 35202 205383   5411 270291 169956]\n",
      "User ID (encoded): tensor([23395])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5024, 0.4845, 0.5401,  ..., 0.4984, 0.4498, 0.5352])\n",
      "Top Item Indices: tensor([37279,  3187, 24763,  6352, 24071])\n",
      "Recommended Item IDs: [407396  35202 270291  70175 262903]\n",
      "User ID (encoded): tensor([67653])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4895, 0.4777, 0.5359,  ..., 0.5045, 0.4530, 0.5335])\n",
      "Top Item Indices: tensor([37279,  7155,  6352, 26272, 22601])\n",
      "Recommended Item IDs: [407396  78476  70175 286647 247224]\n",
      "User ID (encoded): tensor([25143])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4845, 0.4844, 0.5155,  ..., 0.4881, 0.4724, 0.5212])\n",
      "Top Item Indices: tensor([33559, 33237, 37279,  3187, 24071])\n",
      "Recommended Item IDs: [366879 363193 407396  35202 262903]\n",
      "User ID (encoded): tensor([34379])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4885, 0.4826, 0.5312,  ..., 0.5034, 0.4639, 0.5227])\n",
      "Top Item Indices: tensor([37279,  7155,  3187,  6352, 24763])\n",
      "Recommended Item IDs: [407396  78476  35202  70175 270291]\n",
      "User ID (encoded): tensor([16033])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4849, 0.4810, 0.5258,  ..., 0.4986, 0.4852, 0.5188])\n",
      "Top Item Indices: tensor([ 3187, 24763,  6352,  6965,  1935])\n",
      "Recommended Item IDs: [ 35202 270291  70175  76429  21730]\n",
      "User ID (encoded): tensor([64625])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4940, 0.4782, 0.5334,  ..., 0.4984, 0.4568, 0.5403])\n",
      "Top Item Indices: tensor([37279, 24071, 25831, 23773,  3187])\n",
      "Recommended Item IDs: [407396 262903 281826 259847  35202]\n",
      "User ID (encoded): tensor([19243])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4863, 0.4815, 0.5274,  ..., 0.4945, 0.4663, 0.5309])\n",
      "Top Item Indices: tensor([37279, 25831,  3187, 24071,  6352])\n",
      "Recommended Item IDs: [407396 281826  35202 262903  70175]\n",
      "User ID (encoded): tensor([23761])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5034, 0.4753, 0.5373,  ..., 0.4978, 0.4501, 0.5299])\n",
      "Top Item Indices: tensor([ 6829, 25831, 21739,  8323, 37279])\n",
      "Recommended Item IDs: [ 75188 281826 237992  90822 407396]\n",
      "User ID (encoded): tensor([44096])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5110, 0.4632, 0.5546,  ..., 0.4947, 0.4352, 0.5333])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 24763,  6352])\n",
      "Recommended Item IDs: [407396  75188  35202 270291  70175]\n",
      "User ID (encoded): tensor([47767])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4827, 0.4831, 0.5043,  ..., 0.4821, 0.4765, 0.5206])\n",
      "Top Item Indices: tensor([ 3187, 24071, 24763, 12388, 37614])\n",
      "Recommended Item IDs: [ 35202 262903 270291 135446 411198]\n",
      "User ID (encoded): tensor([67029])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5127, 0.4713, 0.5402,  ..., 0.4998, 0.4525, 0.5390])\n",
      "Top Item Indices: tensor([25831,  6829,  3187, 37279,  8323])\n",
      "Recommended Item IDs: [281826  75188  35202 407396  90822]\n",
      "User ID (encoded): tensor([52380])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4815, 0.4801, 0.5204,  ..., 0.4947, 0.4725, 0.5217])\n",
      "Top Item Indices: tensor([33559, 33237,  9660, 37279, 25831])\n",
      "Recommended Item IDs: [366879 363193 105347 407396 281826]\n",
      "User ID (encoded): tensor([2191])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4935, 0.4896, 0.5229,  ..., 0.4945, 0.4710, 0.5238])\n",
      "Top Item Indices: tensor([37279,  3187, 24071, 24763, 15420])\n",
      "Recommended Item IDs: [407396  35202 262903 270291 169956]\n",
      "User ID (encoded): tensor([53958])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4857, 0.4812, 0.5266,  ..., 0.5013, 0.4753, 0.5218])\n",
      "Top Item Indices: tensor([37279, 25831,  3187,  6625, 35718])\n",
      "Recommended Item IDs: [407396 281826  35202  73017 389607]\n",
      "User ID (encoded): tensor([57145])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5270, 0.4837, 0.5571,  ..., 0.5093, 0.4368, 0.5524])\n",
      "Top Item Indices: tensor([ 6829, 37279, 24071,  3187, 15420])\n",
      "Recommended Item IDs: [ 75188 407396 262903  35202 169956]\n",
      "User ID (encoded): tensor([61131])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4920, 0.4785, 0.5256,  ..., 0.4987, 0.4625, 0.5330])\n",
      "Top Item Indices: tensor([25831,  6829, 37279,  6352, 35718])\n",
      "Recommended Item IDs: [281826  75188 407396  70175 389607]\n",
      "User ID (encoded): tensor([40928])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4822, 0.4799, 0.5257,  ..., 0.5017, 0.4812, 0.5216])\n",
      "Top Item Indices: tensor([ 3187, 24763, 15420, 33309,  9357])\n",
      "Recommended Item IDs: [ 35202 270291 169956 364074 102140]\n",
      "User ID (encoded): tensor([64664])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4835, 0.4815, 0.5179,  ..., 0.4887, 0.4676, 0.5274])\n",
      "Top Item Indices: tensor([23773, 37279, 25831,  6352, 24071])\n",
      "Recommended Item IDs: [259847 407396 281826  70175 262903]\n",
      "User ID (encoded): tensor([66264])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5121, 0.4835, 0.5365,  ..., 0.4845, 0.4515, 0.5321])\n",
      "Top Item Indices: tensor([25831, 31182, 35718, 37279, 24071])\n",
      "Recommended Item IDs: [281826 340808 389607 407396 262903]\n",
      "User ID (encoded): tensor([55829])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5210, 0.4700, 0.5736,  ..., 0.4962, 0.4175, 0.5470])\n",
      "Top Item Indices: tensor([37279,  3187, 24763,  3497, 18628])\n",
      "Recommended Item IDs: [407396  35202 270291  38706 205383]\n",
      "User ID (encoded): tensor([2016])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5012, 0.4502, 0.5532,  ..., 0.4815, 0.4164, 0.5273])\n",
      "Top Item Indices: tensor([ 3187,  7155,  1935, 25831, 37279])\n",
      "Recommended Item IDs: [ 35202  78476  21730 281826 407396]\n",
      "User ID (encoded): tensor([29817])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4850, 0.4962, 0.5017,  ..., 0.4966, 0.4919, 0.5057])\n",
      "Top Item Indices: tensor([15420, 40665,  3187,   486, 20182])\n",
      "Recommended Item IDs: [169956 445105  35202   5411 221742]\n",
      "User ID (encoded): tensor([65336])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4848, 0.4850, 0.5129,  ..., 0.4916, 0.4732, 0.5300])\n",
      "Top Item Indices: tensor([ 3187, 24071, 37614, 27581, 21167])\n",
      "Recommended Item IDs: [ 35202 262903 411198 301295 232131]\n",
      "User ID (encoded): tensor([38067])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4881, 0.4866, 0.5259,  ..., 0.5029, 0.4748, 0.5337])\n",
      "Top Item Indices: tensor([ 3187, 15420, 24763, 18628,  9357])\n",
      "Recommended Item IDs: [ 35202 169956 270291 205383 102140]\n",
      "User ID (encoded): tensor([11782])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5121, 0.4808, 0.5500,  ..., 0.5135, 0.4495, 0.5488])\n",
      "Top Item Indices: tensor([37279, 24071,  3187, 26272, 41868])\n",
      "Recommended Item IDs: [407396 262903  35202 286647 458265]\n",
      "User ID (encoded): tensor([14322])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5119, 0.4676, 0.5574,  ..., 0.4958, 0.4373, 0.5369])\n",
      "Top Item Indices: tensor([ 6829, 37279,  3187, 24763,  6352])\n",
      "Recommended Item IDs: [ 75188 407396  35202 270291  70175]\n",
      "User ID (encoded): tensor([63139])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5113, 0.4813, 0.5514,  ..., 0.5126, 0.4523, 0.5452])\n",
      "Top Item Indices: tensor([37279, 24071, 26272, 41868,  3187])\n",
      "Recommended Item IDs: [407396 262903 286647 458265  35202]\n",
      "User ID (encoded): tensor([65941])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5029, 0.4831, 0.5422,  ..., 0.5039, 0.4554, 0.5405])\n",
      "Top Item Indices: tensor([37279,  3187, 24071, 24763,  6352])\n",
      "Recommended Item IDs: [407396  35202 262903 270291  70175]\n",
      "User ID (encoded): tensor([65334])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4956, 0.4824, 0.5363,  ..., 0.5021, 0.4627, 0.5320])\n",
      "Top Item Indices: tensor([37279,  3187, 24071,  7155,  6352])\n",
      "Recommended Item IDs: [407396  35202 262903  78476  70175]\n",
      "User ID (encoded): tensor([53930])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4857, 0.4816, 0.5201,  ..., 0.4965, 0.4638, 0.5313])\n",
      "Top Item Indices: tensor([37279, 24763,  6352, 15420, 24071])\n",
      "Recommended Item IDs: [407396 270291  70175 169956 262903]\n",
      "User ID (encoded): tensor([49265])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5011, 0.4755, 0.5424,  ..., 0.5058, 0.4454, 0.5353])\n",
      "Top Item Indices: tensor([ 6829, 37279,  6352, 24763, 24071])\n",
      "Recommended Item IDs: [ 75188 407396  70175 270291 262903]\n",
      "User ID (encoded): tensor([58999])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4962, 0.4695, 0.5410,  ..., 0.5002, 0.4274, 0.5484])\n",
      "Top Item Indices: tensor([37279,  6829, 15420, 26272, 33309])\n",
      "Recommended Item IDs: [407396  75188 169956 286647 364074]\n",
      "User ID (encoded): tensor([56802])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4790, 0.4890, 0.4967,  ..., 0.4851, 0.4782, 0.5176])\n",
      "Top Item Indices: tensor([15420, 34034, 35655, 37897, 17905])\n",
      "Recommended Item IDs: [169956 372163 389027 414464 197656]\n",
      "User ID (encoded): tensor([55760])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4823, 0.4912, 0.5053,  ..., 0.4891, 0.4776, 0.5242])\n",
      "Top Item Indices: tensor([ 3187, 37279, 24763, 15420, 40665])\n",
      "Recommended Item IDs: [ 35202 407396 270291 169956 445105]\n",
      "User ID (encoded): tensor([41187])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4895, 0.4839, 0.5348,  ..., 0.4987, 0.4660, 0.5355])\n",
      "Top Item Indices: tensor([37279,  3187,  6352,  7155, 24763])\n",
      "Recommended Item IDs: [407396  35202  70175  78476 270291]\n",
      "User ID (encoded): tensor([64675])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4846, 0.4830, 0.5147,  ..., 0.4981, 0.4920, 0.5127])\n",
      "Top Item Indices: tensor([ 3187, 37279, 33559, 31536, 23053])\n",
      "Recommended Item IDs: [ 35202 407396 366879 344902 252244]\n",
      "User ID (encoded): tensor([30093])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5192, 0.4858, 0.5504,  ..., 0.5164, 0.4608, 0.5561])\n",
      "Top Item Indices: tensor([15420, 24763,  3187, 21167, 40715])\n",
      "Recommended Item IDs: [169956 270291  35202 232131 445681]\n",
      "User ID (encoded): tensor([40979])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5019, 0.4787, 0.5367,  ..., 0.5023, 0.4530, 0.5356])\n",
      "Top Item Indices: tensor([ 6829, 37279,  3187, 25831, 24071])\n",
      "Recommended Item IDs: [ 75188 407396  35202 281826 262903]\n",
      "User ID (encoded): tensor([58294])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4915, 0.4777, 0.5296,  ..., 0.5022, 0.4651, 0.5203])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 25831,  7155])\n",
      "Recommended Item IDs: [407396  75188  35202 281826  78476]\n",
      "User ID (encoded): tensor([37920])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4896, 0.4866, 0.5316,  ..., 0.4957, 0.4688, 0.5352])\n",
      "Top Item Indices: tensor([37279,  3187,  7155, 24071, 26272])\n",
      "Recommended Item IDs: [407396  35202  78476 262903 286647]\n",
      "User ID (encoded): tensor([61880])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4940, 0.4825, 0.5248,  ..., 0.4997, 0.4529, 0.5326])\n",
      "Top Item Indices: tensor([37279,  6829, 25831, 35718,  3187])\n",
      "Recommended Item IDs: [407396  75188 281826 389607  35202]\n",
      "User ID (encoded): tensor([55691])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5081, 0.4750, 0.5423,  ..., 0.5075, 0.4512, 0.5398])\n",
      "Top Item Indices: tensor([ 6829, 37279, 24071, 25831,  3187])\n",
      "Recommended Item IDs: [ 75188 407396 262903 281826  35202]\n",
      "User ID (encoded): tensor([17178])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4949, 0.4794, 0.5409,  ..., 0.5075, 0.4569, 0.5328])\n",
      "Top Item Indices: tensor([37279,  7155,  3187, 26272, 22601])\n",
      "Recommended Item IDs: [407396  78476  35202 286647 247224]\n",
      "User ID (encoded): tensor([29335])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4986, 0.4765, 0.5352,  ..., 0.4916, 0.4521, 0.5291])\n",
      "Top Item Indices: tensor([25831, 21739, 35718, 11040, 37279])\n",
      "Recommended Item IDs: [281826 237992 389607 120471 407396]\n",
      "User ID (encoded): tensor([42275])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4936, 0.4724, 0.5355,  ..., 0.4904, 0.4325, 0.5241])\n",
      "Top Item Indices: tensor([ 6829, 25831, 21739,  8323, 35718])\n",
      "Recommended Item IDs: [ 75188 281826 237992  90822 389607]\n",
      "User ID (encoded): tensor([9458])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4906, 0.4722, 0.5393,  ..., 0.5007, 0.4610, 0.5429])\n",
      "Top Item Indices: tensor([37279,  7155,  6829, 22601, 41868])\n",
      "Recommended Item IDs: [407396  78476  75188 247224 458265]\n",
      "User ID (encoded): tensor([58389])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4861, 0.4759, 0.5295,  ..., 0.4979, 0.4640, 0.5337])\n",
      "Top Item Indices: tensor([37279,  6829,  7155, 25831,  6352])\n",
      "Recommended Item IDs: [407396  75188  78476 281826  70175]\n",
      "User ID (encoded): tensor([49107])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5145, 0.4867, 0.5433,  ..., 0.5055, 0.4576, 0.5453])\n",
      "Top Item Indices: tensor([37279, 24071,  3187, 26272, 41868])\n",
      "Recommended Item IDs: [407396 262903  35202 286647 458265]\n",
      "User ID (encoded): tensor([61952])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4890, 0.4695, 0.5270,  ..., 0.4905, 0.4555, 0.5268])\n",
      "Top Item Indices: tensor([33559, 33237, 27581,  6829, 17053])\n",
      "Recommended Item IDs: [366879 363193 301295  75188 188051]\n",
      "User ID (encoded): tensor([10283])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4878, 0.4737, 0.5318,  ..., 0.4983, 0.4622, 0.5374])\n",
      "Top Item Indices: tensor([33559,  6829, 37279, 25831, 33237])\n",
      "Recommended Item IDs: [366879  75188 407396 281826 363193]\n",
      "User ID (encoded): tensor([18817])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5087, 0.4780, 0.5362,  ..., 0.4827, 0.4556, 0.5353])\n",
      "Top Item Indices: tensor([25831,  6829, 37279, 35718, 24071])\n",
      "Recommended Item IDs: [281826  75188 407396 389607 262903]\n",
      "User ID (encoded): tensor([62042])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4840, 0.4805, 0.5189,  ..., 0.4993, 0.4695, 0.5311])\n",
      "Top Item Indices: tensor([37279, 41868,  7155, 33309, 26272])\n",
      "Recommended Item IDs: [407396 458265  78476 364074 286647]\n",
      "User ID (encoded): tensor([46272])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4807, 0.4908, 0.5083,  ..., 0.4934, 0.4760, 0.5256])\n",
      "Top Item Indices: tensor([ 3187, 15420, 40665, 24763, 21167])\n",
      "Recommended Item IDs: [ 35202 169956 445105 270291 232131]\n",
      "User ID (encoded): tensor([17692])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5102, 0.4792, 0.5445,  ..., 0.4992, 0.4470, 0.5389])\n",
      "Top Item Indices: tensor([ 6829, 37279, 24071,  3187,  8323])\n",
      "Recommended Item IDs: [ 75188 407396 262903  35202  90822]\n",
      "User ID (encoded): tensor([68441])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5232, 0.4928, 0.5506,  ..., 0.5055, 0.4478, 0.5357])\n",
      "Top Item Indices: tensor([37279,  3187, 24763, 26272,  6352])\n",
      "Recommended Item IDs: [407396  35202 270291 286647  70175]\n",
      "User ID (encoded): tensor([631])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5120, 0.4701, 0.5519,  ..., 0.5060, 0.4461, 0.5387])\n",
      "Top Item Indices: tensor([ 6829, 37279, 24071,  3187,  6352])\n",
      "Recommended Item IDs: [ 75188 407396 262903  35202  70175]\n",
      "User ID (encoded): tensor([2809])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5120, 0.4765, 0.5521,  ..., 0.5002, 0.4384, 0.5395])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 24763,  6352])\n",
      "Recommended Item IDs: [407396  75188  35202 270291  70175]\n",
      "User ID (encoded): tensor([14167])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5054, 0.4991, 0.5316,  ..., 0.4943, 0.4602, 0.5326])\n",
      "Top Item Indices: tensor([37279,  3187,  7711,  3675, 26272])\n",
      "Recommended Item IDs: [407396  35202  84105  40864 286647]\n",
      "User ID (encoded): tensor([47601])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5173, 0.4643, 0.5515,  ..., 0.5062, 0.4388, 0.5485])\n",
      "Top Item Indices: tensor([25831,  6829, 37279, 35718,  8323])\n",
      "Recommended Item IDs: [281826  75188 407396 389607  90822]\n",
      "User ID (encoded): tensor([28092])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5035, 0.4788, 0.5443,  ..., 0.4985, 0.4496, 0.5335])\n",
      "Top Item Indices: tensor([37279,  3187, 24763,  6352, 24071])\n",
      "Recommended Item IDs: [407396  35202 270291  70175 262903]\n",
      "User ID (encoded): tensor([38699])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5019, 0.4752, 0.5422,  ..., 0.5047, 0.4459, 0.5329])\n",
      "Top Item Indices: tensor([ 6829, 37279,  6352,  3187, 24763])\n",
      "Recommended Item IDs: [ 75188 407396  70175  35202 270291]\n",
      "User ID (encoded): tensor([18445])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4905, 0.4797, 0.5369,  ..., 0.5055, 0.4767, 0.5356])\n",
      "Top Item Indices: tensor([24763,  3187, 15420,  9357, 40665])\n",
      "Recommended Item IDs: [270291  35202 169956 102140 445105]\n",
      "User ID (encoded): tensor([28202])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5044, 0.4721, 0.5395,  ..., 0.5056, 0.4487, 0.5381])\n",
      "Top Item Indices: tensor([ 6829, 25831, 37279, 24071,  3187])\n",
      "Recommended Item IDs: [ 75188 281826 407396 262903  35202]\n",
      "User ID (encoded): tensor([37919])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4840, 0.4763, 0.5270,  ..., 0.4966, 0.4566, 0.5396])\n",
      "Top Item Indices: tensor([37279, 24763, 26272, 22601, 41868])\n",
      "Recommended Item IDs: [407396 270291 286647 247224 458265]\n",
      "User ID (encoded): tensor([16498])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4914, 0.4815, 0.5345,  ..., 0.5022, 0.4647, 0.5302])\n",
      "Top Item Indices: tensor([37279,  7155,  3187,  6352, 26272])\n",
      "Recommended Item IDs: [407396  78476  35202  70175 286647]\n",
      "User ID (encoded): tensor([56355])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5118, 0.4967, 0.5423,  ..., 0.5084, 0.4667, 0.5434])\n",
      "Top Item Indices: tensor([ 3187, 15420, 24763, 12388,   486])\n",
      "Recommended Item IDs: [ 35202 169956 270291 135446   5411]\n",
      "User ID (encoded): tensor([30523])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5054, 0.4659, 0.5403,  ..., 0.5032, 0.4398, 0.5421])\n",
      "Top Item Indices: tensor([ 6829, 25831, 24071,  3187, 37279])\n",
      "Recommended Item IDs: [ 75188 281826 262903  35202 407396]\n",
      "User ID (encoded): tensor([69804])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4959, 0.4652, 0.5416,  ..., 0.4917, 0.4314, 0.5250])\n",
      "Top Item Indices: tensor([37279,  6829, 24763,  6352, 25831])\n",
      "Recommended Item IDs: [407396  75188 270291  70175 281826]\n",
      "User ID (encoded): tensor([18741])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5081, 0.4730, 0.5472,  ..., 0.5005, 0.4303, 0.5522])\n",
      "Top Item Indices: tensor([ 6829, 37279,  6352,  7770, 24763])\n",
      "Recommended Item IDs: [ 75188 407396  70175  84734 270291]\n",
      "User ID (encoded): tensor([23074])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4798, 0.4799, 0.5158,  ..., 0.5014, 0.4891, 0.5119])\n",
      "Top Item Indices: tensor([ 3187, 24763, 15420, 15940, 12388])\n",
      "Recommended Item IDs: [ 35202 270291 169956 175555 135446]\n",
      "User ID (encoded): tensor([44544])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4982, 0.4822, 0.5371,  ..., 0.5025, 0.4679, 0.5295])\n",
      "Top Item Indices: tensor([37279, 25831, 24071,  3187, 37614])\n",
      "Recommended Item IDs: [407396 281826 262903  35202 411198]\n",
      "User ID (encoded): tensor([65155])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4953, 0.4698, 0.5510,  ..., 0.4816, 0.4146, 0.5338])\n",
      "Top Item Indices: tensor([37279,  6829, 24763, 26272, 22601])\n",
      "Recommended Item IDs: [407396  75188 270291 286647 247224]\n",
      "User ID (encoded): tensor([59785])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5219, 0.4762, 0.5603,  ..., 0.5094, 0.4383, 0.5492])\n",
      "Top Item Indices: tensor([ 6829, 37279,  3187, 24071, 24763])\n",
      "Recommended Item IDs: [ 75188 407396  35202 262903 270291]\n",
      "User ID (encoded): tensor([63340])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4945, 0.4787, 0.5245,  ..., 0.4998, 0.4517, 0.5356])\n",
      "Top Item Indices: tensor([ 6829, 37279, 25831, 24071, 35718])\n",
      "Recommended Item IDs: [ 75188 407396 281826 262903 389607]\n",
      "User ID (encoded): tensor([9118])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4816, 0.4852, 0.5193,  ..., 0.5062, 0.4903, 0.5203])\n",
      "Top Item Indices: tensor([ 3187, 36294, 13136, 24763, 12388])\n",
      "Recommended Item IDs: [ 35202 395656 144323 270291 135446]\n",
      "User ID (encoded): tensor([20193])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4996, 0.4772, 0.5313,  ..., 0.5049, 0.4433, 0.5320])\n",
      "Top Item Indices: tensor([ 6829, 25831, 35718, 31182, 37279])\n",
      "Recommended Item IDs: [ 75188 281826 389607 340808 407396]\n",
      "User ID (encoded): tensor([25208])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5154, 0.4765, 0.5471,  ..., 0.5046, 0.4390, 0.5463])\n",
      "Top Item Indices: tensor([ 6829, 37279,  8323, 31182, 24071])\n",
      "Recommended Item IDs: [ 75188 407396  90822 340808 262903]\n",
      "User ID (encoded): tensor([63136])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4990, 0.4691, 0.5308,  ..., 0.4906, 0.4359, 0.5330])\n",
      "Top Item Indices: tensor([ 6829, 37279, 25831, 35718,  6352])\n",
      "Recommended Item IDs: [ 75188 407396 281826 389607  70175]\n",
      "User ID (encoded): tensor([64428])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4873, 0.4848, 0.5270,  ..., 0.4999, 0.4698, 0.5209])\n",
      "Top Item Indices: tensor([37279, 25831,  3187, 35718,  1935])\n",
      "Recommended Item IDs: [407396 281826  35202 389607  21730]\n",
      "User ID (encoded): tensor([47005])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4923, 0.4866, 0.5290,  ..., 0.4943, 0.4648, 0.5287])\n",
      "Top Item Indices: tensor([37279,  3187, 24071, 24763,  6352])\n",
      "Recommended Item IDs: [407396  35202 262903 270291  70175]\n",
      "User ID (encoded): tensor([2461])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5085, 0.4662, 0.5587,  ..., 0.4870, 0.4273, 0.5391])\n",
      "Top Item Indices: tensor([37279,  3187, 24763,  6829,  3675])\n",
      "Recommended Item IDs: [407396  35202 270291  75188  40864]\n",
      "User ID (encoded): tensor([24796])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4943, 0.4817, 0.5336,  ..., 0.5014, 0.4579, 0.5344])\n",
      "Top Item Indices: tensor([37279, 25831,  3187, 35718, 24071])\n",
      "Recommended Item IDs: [407396 281826  35202 389607 262903]\n",
      "User ID (encoded): tensor([48642])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4992, 0.4736, 0.5371,  ..., 0.4933, 0.4395, 0.5234])\n",
      "Top Item Indices: tensor([37279,  6829, 25831, 24763,  3187])\n",
      "Recommended Item IDs: [407396  75188 281826 270291  35202]\n",
      "User ID (encoded): tensor([21071])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4790, 0.4934, 0.5033,  ..., 0.4900, 0.4727, 0.5291])\n",
      "Top Item Indices: tensor([ 3187, 37614, 15420, 21167, 24763])\n",
      "Recommended Item IDs: [ 35202 411198 169956 232131 270291]\n",
      "User ID (encoded): tensor([36936])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4856, 0.4811, 0.5301,  ..., 0.4941, 0.4666, 0.5341])\n",
      "Top Item Indices: tensor([37279,  3187,  7155, 25831, 24071])\n",
      "Recommended Item IDs: [407396  35202  78476 281826 262903]\n",
      "User ID (encoded): tensor([21820])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5331, 0.4845, 0.5673,  ..., 0.5041, 0.4255, 0.5549])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 15420,   486])\n",
      "Recommended Item IDs: [407396  75188  35202 169956   5411]\n",
      "User ID (encoded): tensor([28013])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4829, 0.4919, 0.5100,  ..., 0.4996, 0.4797, 0.5259])\n",
      "Top Item Indices: tensor([ 3187, 24763, 21167, 18628, 15420])\n",
      "Recommended Item IDs: [ 35202 270291 232131 205383 169956]\n",
      "User ID (encoded): tensor([3096])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4935, 0.4796, 0.5274,  ..., 0.4997, 0.4614, 0.5300])\n",
      "Top Item Indices: tensor([ 6829, 37279, 25831, 30265,  6352])\n",
      "Recommended Item IDs: [ 75188 407396 281826 331459  70175]\n",
      "User ID (encoded): tensor([4154])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4837, 0.4832, 0.5188,  ..., 0.4975, 0.4751, 0.5176])\n",
      "Top Item Indices: tensor([ 3187, 37279,  6352, 33559, 29095])\n",
      "Recommended Item IDs: [ 35202 407396  70175 366879 318333]\n",
      "User ID (encoded): tensor([11485])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4820, 0.4898, 0.5160,  ..., 0.4965, 0.4755, 0.5255])\n",
      "Top Item Indices: tensor([ 3187, 15420, 24763, 21167, 18628])\n",
      "Recommended Item IDs: [ 35202 169956 270291 232131 205383]\n",
      "User ID (encoded): tensor([69967])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4717, 0.4685, 0.5162,  ..., 0.4842, 0.4627, 0.5316])\n",
      "Top Item Indices: tensor([33559, 23773, 37279, 25831, 27555])\n",
      "Recommended Item IDs: [366879 259847 407396 281826 301019]\n",
      "User ID (encoded): tensor([49664])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4864, 0.4879, 0.5204,  ..., 0.4974, 0.4730, 0.5184])\n",
      "Top Item Indices: tensor([33559, 33237, 37279, 11214,  3187])\n",
      "Recommended Item IDs: [366879 363193 407396 122546  35202]\n",
      "User ID (encoded): tensor([54482])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4731, 0.4873, 0.4949,  ..., 0.4909, 0.4856, 0.5099])\n",
      "Top Item Indices: tensor([15420, 40665, 17905,  9469,   486])\n",
      "Recommended Item IDs: [169956 445105 197656 103342   5411]\n",
      "User ID (encoded): tensor([21685])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4865, 0.4862, 0.5279,  ..., 0.4985, 0.4727, 0.5225])\n",
      "Top Item Indices: tensor([37279,  3187, 24071,  6352, 33559])\n",
      "Recommended Item IDs: [407396  35202 262903  70175 366879]\n",
      "User ID (encoded): tensor([29283])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4883, 0.4783, 0.5357,  ..., 0.5048, 0.4511, 0.5336])\n",
      "Top Item Indices: tensor([37279,  6352,  7155, 26272, 24763])\n",
      "Recommended Item IDs: [407396  70175  78476 286647 270291]\n",
      "User ID (encoded): tensor([40021])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5006, 0.4774, 0.5428,  ..., 0.5056, 0.4600, 0.5464])\n",
      "Top Item Indices: tensor([37279, 24071, 25831,  3187,  7155])\n",
      "Recommended Item IDs: [407396 262903 281826  35202  78476]\n",
      "User ID (encoded): tensor([63104])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4920, 0.4834, 0.5334,  ..., 0.5054, 0.4747, 0.5324])\n",
      "Top Item Indices: tensor([33309,  3187, 24763, 37279, 15420])\n",
      "Recommended Item IDs: [364074  35202 270291 407396 169956]\n",
      "User ID (encoded): tensor([50533])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4858, 0.4870, 0.5164,  ..., 0.5011, 0.4939, 0.5158])\n",
      "Top Item Indices: tensor([ 3187, 24763, 16607, 31536, 13136])\n",
      "Recommended Item IDs: [ 35202 270291 183066 344902 144323]\n",
      "User ID (encoded): tensor([24766])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4968, 0.4901, 0.5265,  ..., 0.5015, 0.4677, 0.5340])\n",
      "Top Item Indices: tensor([37279,  3187, 26272, 24763, 33309])\n",
      "Recommended Item IDs: [407396  35202 286647 270291 364074]\n",
      "User ID (encoded): tensor([16166])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4932, 0.4804, 0.5231,  ..., 0.4981, 0.4541, 0.5327])\n",
      "Top Item Indices: tensor([37279,  6829, 25831, 24763, 24071])\n",
      "Recommended Item IDs: [407396  75188 281826 270291 262903]\n",
      "User ID (encoded): tensor([24444])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4939, 0.4862, 0.5294,  ..., 0.4974, 0.4649, 0.5279])\n",
      "Top Item Indices: tensor([37279,  3187, 25831,  1935, 24071])\n",
      "Recommended Item IDs: [407396  35202 281826  21730 262903]\n",
      "User ID (encoded): tensor([17185])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4908, 0.4812, 0.5305,  ..., 0.4948, 0.4734, 0.5347])\n",
      "Top Item Indices: tensor([24056, 37279,  8323,  7155,  6625])\n",
      "Recommended Item IDs: [262752 407396  90822  78476  73017]\n",
      "User ID (encoded): tensor([32830])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4957, 0.4809, 0.5330,  ..., 0.5025, 0.4535, 0.5290])\n",
      "Top Item Indices: tensor([37279,  6829,  6352, 24071, 24763])\n",
      "Recommended Item IDs: [407396  75188  70175 262903 270291]\n",
      "User ID (encoded): tensor([53598])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4844, 0.4776, 0.5186,  ..., 0.4958, 0.4718, 0.5203])\n",
      "Top Item Indices: tensor([33559,  3187, 28166,  6829, 41945])\n",
      "Recommended Item IDs: [366879  35202 307695  75188 459122]\n",
      "User ID (encoded): tensor([65265])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4901, 0.4838, 0.5260,  ..., 0.4928, 0.4756, 0.5343])\n",
      "Top Item Indices: tensor([24056, 37279,  8323,  6496, 13136])\n",
      "Recommended Item IDs: [262752 407396  90822  71698 144323]\n",
      "User ID (encoded): tensor([38351])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5052, 0.4850, 0.5396,  ..., 0.5013, 0.4589, 0.5338])\n",
      "Top Item Indices: tensor([37279,  3187, 24071, 24763,  6352])\n",
      "Recommended Item IDs: [407396  35202 262903 270291  70175]\n",
      "User ID (encoded): tensor([46233])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5291, 0.4756, 0.5682,  ..., 0.5016, 0.4242, 0.5643])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 15420,  3675])\n",
      "Recommended Item IDs: [407396  75188  35202 169956  40864]\n",
      "User ID (encoded): tensor([16660])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4928, 0.4886, 0.5208,  ..., 0.4986, 0.4671, 0.5181])\n",
      "Top Item Indices: tensor([ 3187, 25831, 37279,  1935, 35718])\n",
      "Recommended Item IDs: [ 35202 281826 407396  21730 389607]\n",
      "User ID (encoded): tensor([68147])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5064, 0.4743, 0.5464,  ..., 0.4991, 0.4425, 0.5376])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 24763,  6352])\n",
      "Recommended Item IDs: [407396  75188  35202 270291  70175]\n",
      "User ID (encoded): tensor([29560])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4962, 0.4813, 0.5297,  ..., 0.4986, 0.4538, 0.5371])\n",
      "Top Item Indices: tensor([ 6829, 37279, 24071, 25831,  3187])\n",
      "Recommended Item IDs: [ 75188 407396 262903 281826  35202]\n",
      "User ID (encoded): tensor([48039])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5092, 0.4684, 0.5523,  ..., 0.5016, 0.4415, 0.5379])\n",
      "Top Item Indices: tensor([37279,  7155,  6352,  3187, 41868])\n",
      "Recommended Item IDs: [407396  78476  70175  35202 458265]\n",
      "User ID (encoded): tensor([62395])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5291, 0.4801, 0.5553,  ..., 0.4963, 0.4363, 0.5418])\n",
      "Top Item Indices: tensor([37279,  3187, 25831,  7711,   486])\n",
      "Recommended Item IDs: [407396  35202 281826  84105   5411]\n",
      "User ID (encoded): tensor([9944])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5142, 0.4786, 0.5396,  ..., 0.4880, 0.4479, 0.5349])\n",
      "Top Item Indices: tensor([25831,  3187, 35718, 11040,  1935])\n",
      "Recommended Item IDs: [281826  35202 389607 120471  21730]\n",
      "User ID (encoded): tensor([16731])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5010, 0.4752, 0.5420,  ..., 0.4993, 0.4442, 0.5265])\n",
      "Top Item Indices: tensor([37279,  6829,  3187, 24763,  6352])\n",
      "Recommended Item IDs: [407396  75188  35202 270291  70175]\n",
      "User ID (encoded): tensor([50426])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4986, 0.4774, 0.5467,  ..., 0.5100, 0.4582, 0.5378])\n",
      "Top Item Indices: tensor([37279,  7155, 41868, 26272, 33309])\n",
      "Recommended Item IDs: [407396  78476 458265 286647 364074]\n",
      "User ID (encoded): tensor([47899])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5017, 0.4851, 0.5401,  ..., 0.5045, 0.4662, 0.5371])\n",
      "Top Item Indices: tensor([37279, 26272, 41868,  3187, 33309])\n",
      "Recommended Item IDs: [407396 286647 458265  35202 364074]\n",
      "User ID (encoded): tensor([49722])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.4862, 0.4928, 0.5109,  ..., 0.5016, 0.4815, 0.5275])\n",
      "Top Item Indices: tensor([ 3187, 18628, 24763, 36294, 21167])\n",
      "Recommended Item IDs: [ 35202 205383 270291 395656 232131]\n",
      "User ID (encoded): tensor([45839])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5029, 0.4677, 0.5498,  ..., 0.4965, 0.4315, 0.5413])\n",
      "Top Item Indices: tensor([ 6829, 37279, 24763,  6352, 15420])\n",
      "Recommended Item IDs: [ 75188 407396 270291  70175 169956]\n",
      "User ID (encoded): tensor([51694])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5046, 0.4640, 0.5394,  ..., 0.5117, 0.4484, 0.5446])\n",
      "Top Item Indices: tensor([ 6829, 37279,  3187, 25831,   486])\n",
      "Recommended Item IDs: [ 75188 407396  35202 281826   5411]\n",
      "User ID (encoded): tensor([36741])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5039, 0.4825, 0.5417,  ..., 0.5085, 0.4568, 0.5498])\n",
      "Top Item Indices: tensor([37279, 26272, 33309,  3187, 21167])\n",
      "Recommended Item IDs: [407396 286647 364074  35202 232131]\n",
      "User ID (encoded): tensor([66230])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5051, 0.4670, 0.5457,  ..., 0.5022, 0.4345, 0.5447])\n",
      "Top Item Indices: tensor([ 6829, 25831, 11040, 24071, 35718])\n",
      "Recommended Item IDs: [ 75188 281826 120471 262903 389607]\n",
      "User ID (encoded): tensor([34521])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5212, 0.4692, 0.5526,  ..., 0.5068, 0.4298, 0.5531])\n",
      "Top Item Indices: tensor([ 6829, 25831,  7770, 37279, 35718])\n",
      "Recommended Item IDs: [ 75188 281826  84734 407396 389607]\n",
      "User ID (encoded): tensor([13783])\n",
      "Item Indices: tensor([    0,     1,     2,  ..., 42625, 42626, 42627])\n",
      "Scores: tensor([0.5019, 0.4680, 0.5453,  ..., 0.5011, 0.4374, 0.5372])\n",
      "Top Item Indices: tensor([25831, 11040, 35718,  6829, 37279])\n",
      "Recommended Item IDs: [281826 120471 389607  75188 407396]\n",
      "Precision@5: 0.0\n",
      "Recall@5: 0.0\n",
      "Hits@5: 0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "precision, recall, hits = evaluate_model(test_df_copy, num_recommendations=5)\n",
    "\n",
    "print(f'Precision@5: {precision}')\n",
    "print(f'Recall@5: {recall}')\n",
    "print(f'Hits@5: {hits}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11b1f0-01de-46c5-8cdf-0826eb4f1c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
